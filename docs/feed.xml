<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2024-03-31T19:52:38+03:00</updated><id>/feed.xml</id><title type="html">Jacob Shapira | Software architecture, engineering and AI</title><subtitle></subtitle><entry><title type="html">Visualizing data processing progress with tqdm</title><link href="/jekyll/update/2024/03/30/visualizing-data-processing-with-tqdm.html" rel="alternate" type="text/html" title="Visualizing data processing progress with tqdm" /><published>2024-03-30T12:31:29+03:00</published><updated>2024-03-30T12:31:29+03:00</updated><id>/jekyll/update/2024/03/30/visualizing-data-processing-with-tqdm</id><content type="html" xml:base="/jekyll/update/2024/03/30/visualizing-data-processing-with-tqdm.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#intro" id="markdown-toc-intro">Intro</a></li>
  <li><a href="#integrations-advanced-features-and-performance" id="markdown-toc-integrations-advanced-features-and-performance">Integrations, advanced features and performance</a></li>
</ul>

<h3 id="intro">Intro</h3>
<p>This entry would be just a quick tip for those of you who are working with data processing in Python and would benefit from a visualizing the progress.
There’s a simple library called <code class="language-plaintext highlighter-rouge">tqdm</code> which allow you to do that with a minimal overhead.</p>

<p>Let’s check the usage with a model training dummy code:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>

<span class="n">train</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">data</span><span class="p">:</span> <span class="p">[</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s">"Training"</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">)]</span>
<span class="n">validate</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">data</span><span class="p">:</span> <span class="p">[</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s">"Validating"</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">)]</span>
<span class="n">load_data</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>

<span class="n">all_data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>

<span class="n">batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">all_data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">100</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_data</span><span class="p">),</span> <span class="mi">100</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">batches</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s">"Processing batch"</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">validate</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></code></pre></figure>

<p>As you can see, we have 3 dummy functions for loading data, training and validating, each of them invoking <code class="language-plaintext highlighter-rouge">sleep</code> function to simulate data processing.<br />
The main code consists of batch processing of the training data.</p>

<p>If we run the code, we’ll be able to see the progress of the batches, the training and the validation of each batch, separately.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">Processing batch:   0%|          | 0/10 <span class="o">[</span>00:00&lt;?, ?it/s]

Training:   0%|          | 0/100 <span class="o">[</span>00:00&lt;?, ?it/s]
Training:  10%|█         | 10/100 <span class="o">[</span>00:00&lt;00:00, 95.57it/s]
Training:  20%|██        | 20/100 <span class="o">[</span>00:00&lt;00:00, 96.27it/s]
Training:  30%|███       | 30/100 <span class="o">[</span>00:00&lt;00:00, 95.79it/s]
Training:  40%|████      | 40/100 <span class="o">[</span>00:00&lt;00:00, 95.70it/s]
Training:  50%|█████     | 50/100 <span class="o">[</span>00:00&lt;00:00, 95.16it/s]
Training:  60%|██████    | 60/100 <span class="o">[</span>00:00&lt;00:00, 95.29it/s]
Training:  70%|███████   | 70/100 <span class="o">[</span>00:00&lt;00:00, 94.88it/s]
Training:  80%|████████  | 80/100 <span class="o">[</span>00:00&lt;00:00, 94.61it/s]
Training:  90%|█████████ | 90/100 <span class="o">[</span>00:00&lt;00:00, 94.76it/s]
Training: 100%|██████████| 100/100 <span class="o">[</span>00:01&lt;00:00, 94.72it/s]
                                                           
Validating:   0%|          | 0/100 <span class="o">[</span>00:00&lt;?, ?it/s]
Validating:  10%|█         | 10/100 <span class="o">[</span>00:00&lt;00:00, 95.87it/s]
Validating:  20%|██        | 20/100 <span class="o">[</span>00:00&lt;00:00, 95.28it/s]
Validating:  30%|███       | 30/100 <span class="o">[</span>00:00&lt;00:00, 95.82it/s]
Validating:  40%|████      | 40/100 <span class="o">[</span>00:00&lt;00:00, 95.41it/s]
Validating:  50%|█████     | 50/100 <span class="o">[</span>00:00&lt;00:00, 95.66it/s]
Validating:  60%|██████    | 60/100 <span class="o">[</span>00:00&lt;00:00, 95.02it/s]
Validating:  70%|███████   | 70/100 <span class="o">[</span>00:00&lt;00:00, 95.23it/s]
Validating:  80%|████████  | 80/100 <span class="o">[</span>00:00&lt;00:00, 94.96it/s]
Validating:  90%|█████████ | 90/100 <span class="o">[</span>00:00&lt;00:00, 94.84it/s]
Validating: 100%|██████████| 100/100 <span class="o">[</span>00:01&lt;00:00, 94.92it/s]

Processing batch:  10%|█         | 1/10 <span class="o">[</span>00:02&lt;00:18,  2.10s/it]

Training:   0%|          | 0/100 <span class="o">[</span>00:00&lt;?, ?it/s]
Training:  10%|█         | 10/100 <span class="o">[</span>00:00&lt;00:00, 94.69it/s]
Training:  20%|██        | 20/100 <span class="o">[</span>00:00&lt;00:00, 95.85it/s]
Training:  30%|███       | 30/100 <span class="o">[</span>00:00&lt;00:00, 95.53it/s]
Training:  40%|████      | 40/100 <span class="o">[</span>00:00&lt;00:00, 94.67it/s]
Training:  50%|█████     | 50/100 <span class="o">[</span>00:00&lt;00:00, 95.10it/s]
Training:  60%|██████    | 60/100 <span class="o">[</span>00:00&lt;00:00, 95.35it/s]
Training:  70%|███████   | 70/100 <span class="o">[</span>00:00&lt;00:00, 95.70it/s]
Training:  80%|████████  | 80/100 <span class="o">[</span>00:00&lt;00:00, 96.15it/s]
Training:  90%|█████████ | 90/100 <span class="o">[</span>00:00&lt;00:00, 96.05it/s]
Training: 100%|██████████| 100/100 <span class="o">[</span>00:01&lt;00:00, 95.77it/s]
                                                           
Validating:   0%|          | 0/100 <span class="o">[</span>00:00&lt;?, ?it/s]
Validating:  10%|█         | 10/100 <span class="o">[</span>00:00&lt;00:00, 96.14it/s]
Validating:  20%|██        | 20/100 <span class="o">[</span>00:00&lt;00:00, 96.19it/s]
Validating:  30%|███       | 30/100 <span class="o">[</span>00:00&lt;00:00, 96.02it/s]
Validating:  40%|████      | 40/100 <span class="o">[</span>00:00&lt;00:00, 96.20it/s]
Validating:  50%|█████     | 50/100 <span class="o">[</span>00:00&lt;00:00, 95.70it/s]
Validating:  60%|██████    | 60/100 <span class="o">[</span>00:00&lt;00:00, 95.70it/s]
Validating:  70%|███████   | 70/100 <span class="o">[</span>00:00&lt;00:00, 95.40it/s]
Validating:  80%|████████  | 80/100 <span class="o">[</span>00:00&lt;00:00, 94.90it/s]
Validating:  90%|█████████ | 90/100 <span class="o">[</span>00:00&lt;00:00, 95.30it/s]
Validating: 100%|██████████| 100/100 <span class="o">[</span>00:01&lt;00:00, 95.33it/s]

........................</code></pre></figure>

<h3 id="integrations-advanced-features-and-performance">Integrations, advanced features and performance</h3>
<p>tqdm supports Pandas, Keras, Dask and IPython/Jupyter integrations and provides advances features such as manual updates of the progress,
callbacks support, custom messages, formatting and more.<br />
On top of that, even though tqdm claims to add only 60ns per iteration (and 80ns for gui), one of the more important features is the control over performance related properties such as intervals and miniters.</p>

<p>Refer to <a href="https://github.com/tqdm/tqdm" target="_blank">its Github page</a> for the full docs.</p>]]></content><author><name>Jacob Shapira</name></author><category term="jekyll" /><category term="update" /><category term="BackEnd" /><category term="DataEngineering" /><category term="LLM/ML/AI" /></entry><entry><title type="html">Improving consistency in distributed applications using Saga pattern with Orkes (Netflix) conductor</title><link href="/jekyll/update/2024/01/28/mircroservice-transactions.html" rel="alternate" type="text/html" title="Improving consistency in distributed applications using Saga pattern with Orkes (Netflix) conductor" /><published>2024-01-28T11:31:29+02:00</published><updated>2024-01-28T11:31:29+02:00</updated><id>/jekyll/update/2024/01/28/mircroservice-transactions</id><content type="html" xml:base="/jekyll/update/2024/01/28/mircroservice-transactions.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#intro" id="markdown-toc-intro">Intro</a></li>
  <li><a href="#saga-pattern" id="markdown-toc-saga-pattern">Saga Pattern</a></li>
  <li><a href="#orkes-netflix-conductor-as-infrastructure-level-solution" id="markdown-toc-orkes-netflix-conductor-as-infrastructure-level-solution">Orkes (Netflix) Conductor as infrastructure level solution</a></li>
  <li><a href="#compensation-transactions" id="markdown-toc-compensation-transactions">Compensation transactions</a></li>
</ul>

<h3 id="intro">Intro</h3>
<p>Microservice architecture has become the standard for most complex systems.
While it addresses many issues, it also introduces certain complexities.
One such complexity is maintaining ACID principles in distributed flows across multiple services.<br />
A basic example involves a product purchase flow, requiring:</p>

<ul>
  <li>Inventory verification (Inventory Service)</li>
  <li>Payment processing (Payment Service)</li>
  <li>Product shipment submission (Shipment Service)</li>
</ul>

<p>Consider a situation where both the inventory verification and payment processing succeed, but your request is denied by the Shipment Service.<br />
Assume, for instance, the rejection stems from a sudden change in shipping policies that disallows delivery to the provided customer address, rather than from some technical reason,
which theoretically can allow us to retry instead of failing a flow.<br />
In such circumstances, you would likely need to process a refund and possibly inform the Inventory Service that the item is available again.<br />
This scenario highlights a particular challenge, but that there may be additional, similar situations. 
Therefore, a more general, infrastructure-level solution is necessary to manage failures within distributed workflows effectively.</p>

<h3 id="saga-pattern">Saga Pattern</h3>
<p>In simple terms, the Saga pattern is utilized to maintain consistency in distributed applications through coordination and failures management. 
A Saga consists of a series of transactions that are initiated either by a central coordinator (Orchestrated) or through a reactive approach by responding to events (Choreographed).<br />
When any transaction within the Saga fails, a compensation transaction is activated to revert the application back to a consistent state.<br />
Taking our example (assuming we’re using a coordinator), if the shipment service transaction fails, 
the coordinator will initiate a refund transaction with the payment service and update the inventory status with the inventory service.</p>

<h3 id="orkes-netflix-conductor-as-infrastructure-level-solution">Orkes (Netflix) Conductor as infrastructure level solution</h3>
<p>Conductor is a microservice orchestration tool created by Netflix and later forked by Orkes.<br />
Given the right circumstances, it is a good solution if you’re looking to implement a generic, infrastructure-level solution to orchestrate your distributed flows.<br />
It supports creating and managing complex applicative flows, including timeouts, retries and a lot of useful features, among them - failure handling.<br />
With that being said, it is not without caveats, starting from maintaining additional infrastructure to the need for careful workflow design (e.g, not passing large payloads between workflows).</p>

<h3 id="compensation-transactions">Compensation transactions</h3>
<p>Conductor offers a concept of a <a href="https://orkes.io/content/error-handling" target="_blank">failureWorkflow</a>.<br />
Failure workflows can be added to your distributed workflows and triggered if one of the transaction in the Saga did not successfully complete.<br />
Conductor interface provides a clear visibility over the workflow itself and the routes it can take whether succeeded or failed, making it a solid choice if you’re looking for a
battle tested infrastructure level microservice orchestration solution.</p>]]></content><author><name>Jacob Shapira</name></author><category term="jekyll" /><category term="update" /><category term="BackEnd" /><category term="Architecture" /><category term="Cloud" /></entry><entry><title type="html">Effective LLM fine-tuning for dummies</title><link href="/jekyll/update/2023/09/18/llm-fine-tuning-hypterparameters-simplified.html" rel="alternate" type="text/html" title="Effective LLM fine-tuning for dummies" /><published>2023-09-18T12:31:29+03:00</published><updated>2023-09-18T12:31:29+03:00</updated><id>/jekyll/update/2023/09/18/llm-fine-tuning-hypterparameters-simplified</id><content type="html" xml:base="/jekyll/update/2023/09/18/llm-fine-tuning-hypterparameters-simplified.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#intro" id="markdown-toc-intro">Intro</a></li>
  <li><a href="#base-model-fine-tuned-model-and-initial-weight-and-biases" id="markdown-toc-base-model-fine-tuned-model-and-initial-weight-and-biases">Base model, fine-tuned model and initial weight and biases</a></li>
  <li><a href="#preparing-the-base-model" id="markdown-toc-preparing-the-base-model">Preparing the base model</a></li>
  <li><a href="#creating-domain-specific-dataset" id="markdown-toc-creating-domain-specific-dataset">Creating domain specific dataset</a></li>
  <li><a href="#preparing-data-sets" id="markdown-toc-preparing-data-sets">Preparing data sets</a>    <ul>
      <li><a href="#data-split-with-a-seed---train-test-and-validation-sets" id="markdown-toc-data-split-with-a-seed---train-test-and-validation-sets">Data split with a seed - train, test and validation sets</a></li>
      <li><a href="#tensors-and-pytorch-models" id="markdown-toc-tensors-and-pytorch-models">Tensors and PyTorch models</a></li>
      <li><a href="#max-sequence-length" id="markdown-toc-max-sequence-length">Max sequence length</a></li>
    </ul>
  </li>
  <li><a href="#starting-the-actual-fine-tuning" id="markdown-toc-starting-the-actual-fine-tuning">Starting the actual fine tuning</a>    <ul>
      <li><a href="#fine-tuning-flow" id="markdown-toc-fine-tuning-flow">Fine-tuning flow</a></li>
      <li><a href="#learning-rate-and-catastrophic-forgetting" id="markdown-toc-learning-rate-and-catastrophic-forgetting">Learning rate and catastrophic forgetting</a></li>
      <li><a href="#epochs-and-epoch-sizes" id="markdown-toc-epochs-and-epoch-sizes">Epochs and epoch sizes</a></li>
      <li><a href="#gradient-accumulation-batches-and-micro-batches" id="markdown-toc-gradient-accumulation-batches-and-micro-batches">Gradient accumulation, batches and micro-batches</a></li>
      <li><a href="#overfitting-underfitting-and-weight-decay" id="markdown-toc-overfitting-underfitting-and-weight-decay">Overfitting, underfitting and weight decay</a></li>
      <li><a href="#warmup-iterations" id="markdown-toc-warmup-iterations">Warmup iterations</a></li>
    </ul>
  </li>
  <li><a href="#running-inference" id="markdown-toc-running-inference">Running inference</a></li>
</ul>

<h3 id="intro">Intro</h3>
<p>At the time of writing this entry, the tech world is caught up in what I like to call a LLM-BOOM.<br />
Everyone’s implementing LLM based features, new models announced on an hourly basis and the LLM ecosystem, both open source and proprietary is exploding.<br />
A lot of professionals (such as software engineers and architects) which aren’t specialized in AI and data science domains required to gain
sufficient knowledge to enable and lead features and projects in these domains.<br />
Luckily, one of the advantages of this LLM-BOOM is that tools are getting better and easier to use.</p>

<p>One of the most frequent tasks is to create LLMs with domain specific knowledge,
and the purpose of this post is to explain LLM fine tuning process, terminology and hyperparameters in simple terms for professionals
which aren’t extremely proficient the data science and AI domains.</p>

<p>Fine-tuning an LLM model was not a very straightforward task a year ago, but now we have variety of tools allowing step-by-step fine tuning of models.
One of these tools is <a href="https://github.com/Lightning-AI/lit-gpt" target="_blank">Lit-GPT</a>, which is defined as an <code class="language-plaintext highlighter-rouge">Hackable implementation of state-of-the-art open-source large language models released under the Apache 2.0 license</code>.
Which basically means it’s main design goal is simplicity over SOLID principles.</p>

<p>With that being said, in order to use this tool (or any other for that matter) effectively, you need to properly configure it, and in order to properly configure it,
you need to understand what each configuration means and have sufficient general knowledge about what you’re doing.</p>

<p>I will use Lit-GPT as an example tool due to creators decision to choose simplicity over other design properties,
but the concepts and parameters this entry will cover are generic to many fine tuning other tools out there.</p>

<h3 id="base-model-fine-tuned-model-and-initial-weight-and-biases">Base model, fine-tuned model and initial weight and biases</h3>
<p>In most use cases, when training a model from scratch, weights and biases are initialized with either random or constant values
and does not truthfully represent the data.</p>

<p>During the training process the weights and biases are adjusted and the <code class="language-plaintext highlighter-rouge">base model</code> gains understanding of natural language, grammar and the data it was trained on.<br />
When fine tuning, you’ll (usually) use a <code class="language-plaintext highlighter-rouge">base model</code> with pre-existing weights and biases, and train it on your domain-specific datasets to adjust the (but not re-generate) the weights.</p>

<p>The model with the updated weights and biases is the <code class="language-plaintext highlighter-rouge">fine tuned model</code>.<br />
As you probably understand, fine tuning a model is much simpler, quicker and cheaper than training a model from scratch.<br />
Of course there is an exception if you attempt to train a base model on a domain specific data, but this is a more complex
use case and out of scope for this post.</p>

<h3 id="preparing-the-base-model">Preparing the base model</h3>
<p>Now that we understand the difference between base and fine-tuned models, we need to select the base model
that we will fine tune.<br />
Lit-GPT supports 16 open source models (you can see the full list in the Github repo),
but we’ll run our example based on falcon model.</p>

<p>It’s pretty straightforward to download base model checkpoints and convert it from Hugging Face to Lit-GPT.<br />
If you’re not familiar with <a href="https://huggingface.co/" target="_blank">HF</a>, it’s a collaboration platform for te
AI &amp; LLM community, I like to think of it as Github for AI.</p>

<p>You’ll need to run the following:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Download the falcon7b-instruct base model:
</span><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">download</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">repo_id</span> <span class="n">tiiuae</span><span class="o">/</span><span class="n">falcon</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">instruct</span>

<span class="c1"># Convert from HF to Lit-GPT format:  
</span><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">convert_hf_checkpoint</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">checkpoint_dir</span> <span class="n">checkpoints</span><span class="o">/</span><span class="n">tiiuae</span><span class="o">/</span><span class="n">falcon</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">instruct</span>

<span class="c1"># Run test inference  
</span><span class="n">python</span> <span class="n">generate</span><span class="o">/</span><span class="n">base</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">prompt</span> <span class="s">"Annie, are you ok?"</span> <span class="o">--</span><span class="n">checkpoint_dir</span> <span class="n">checkpoints</span><span class="o">/</span><span class="n">tiiuae</span><span class="o">/</span><span class="n">falcon</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span></code></pre></figure>

<p>Now that we have our base model up and running, we can move forward.</p>

<h3 id="creating-domain-specific-dataset">Creating domain specific dataset</h3>
<p>When fine tuning general purpose models with domain specific knowledge, most of the time (but not always) you’ll need to create some sort of input-output datasets.<br />
It can be instructions with inputs and outputs, questions and answers or other formats.<br />
In our case the fine tuning will be done on an instruction data set (which means the more specific term in our case is instruction tuning, but let’s ignore that for simplicity purposes)
based on the <a href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank">Alpaca</a> dataset, which looks like this:</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">[</span><span class="w">
</span><span class="err">.....</span><span class="w">
    </span><span class="p">{</span><span class="w">
        </span><span class="nl">"instruction"</span><span class="p">:</span><span class="w"> </span><span class="s2">"What are the three primary colors?"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"input"</span><span class="p">:</span><span class="w"> </span><span class="s2">""</span><span class="p">,</span><span class="w">
        </span><span class="nl">"output"</span><span class="p">:</span><span class="w"> </span><span class="s2">"The three primary colors are red, blue, and yellow."</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="w">
        </span><span class="nl">"instruction"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Evaluate this sentence for spelling and grammar mistakes"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"input"</span><span class="p">:</span><span class="w"> </span><span class="s2">"He finnished his meal and left the resturant"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"output"</span><span class="p">:</span><span class="w"> </span><span class="s2">"He finished his meal and left the restaurant."</span><span class="w">
    </span><span class="p">},</span><span class="w">
</span><span class="err">.....</span><span class="w">
</span><span class="p">]</span></code></pre></figure>

<p>To proceed with our use case, you should create a domain specific dataset in a similar format:</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">[</span><span class="w">
</span><span class="err">.....</span><span class="w">
    </span><span class="p">{</span><span class="w">
        </span><span class="nl">"instruction"</span><span class="p">:</span><span class="w"> </span><span class="s2">"My instruction..."</span><span class="p">,</span><span class="w">
        </span><span class="nl">"input"</span><span class="p">:</span><span class="w"> </span><span class="s2">"My input..."</span><span class="p">,</span><span class="w">
        </span><span class="nl">"output"</span><span class="p">:</span><span class="w"> </span><span class="s2">"My output..."</span><span class="p">,</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="err">.....</span><span class="w">
</span><span class="p">]</span></code></pre></figure>

<h3 id="preparing-data-sets">Preparing data sets</h3>
<p>After creating the data set, we can run the preparation process.<br />
If you examine the <code class="language-plaintext highlighter-rouge">scripts/prepare_alpaca.py</code> file, you’ll be able to see that you can set various parameters.<br />
While most of them are pretty self-explanatory (destination path, checkpoint dir etc’), some of them deserve further explanation.</p>

<h4 id="data-split-with-a-seed---train-test-and-validation-sets">Data split with a seed - train, test and validation sets</h4>
<p>One of the things you’ll need to define is the <code class="language-plaintext highlighter-rouge">test_split_size</code> which indicates the size of the data you’ll use to test the performance.
In this case we have only training and test sets, but it is a common practice to use also validation data sets.<br />
The optimal train/validation/test split depends on your data, but standard splits can be 80/10/10, 70/15/15 or 60/20/20.</p>

<p><code class="language-plaintext highlighter-rouge">Training data</code> is pretty self-explanatory, this is the data you’ll be using to train your model.</p>

<p><code class="language-plaintext highlighter-rouge">Validation data</code> used to automatically or manually optimize hyperparameters during training phase.
For example, every N intervals, we can validate the model performance with a loss function.</p>

<p>A <code class="language-plaintext highlighter-rouge">loss function</code> quantifies the difference between the predicated output as stated in the validation samples and the actual output produced by the model.<br />
If we (manually or automatically) notice that the loss function stops decreasing (or stops increasing), we might want to adjust the hyperparameters.</p>

<p><code class="language-plaintext highlighter-rouge">Test data</code> is the final data on which we will test the model performance.</p>

<p>You’ll also be required to set a <code class="language-plaintext highlighter-rouge">seed</code>.<br />
A seed usually used in order to reproduce random generation, so for a specific seed, the random sequence will always be the same.<br />
In our case, it is used to take the same random order of <code class="language-plaintext highlighter-rouge">test_split_size</code> samples from the data set.
Although the test samples are randomized, it is important to use seed for reproducibility of experiments.
The actual number of the seed is not important as long as it is consistent per experiment.</p>

<h4 id="tensors-and-pytorch-models">Tensors and PyTorch models</h4>
<p>As you’ll see once you run the preparation script, you’ll get <code class="language-plaintext highlighter-rouge">train.pt</code> and <code class="language-plaintext highlighter-rouge">test.pt</code> file in your <code class="language-plaintext highlighter-rouge">/data</code> folder.</p>
<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">.pt</code> files are saved PyTorch models.</p>
  </li>
  <li>
    <p>PyTorch models can store various data, such as weights, tokens, simple python objects and tensors.</p>
  </li>
  <li>
    <p>A <code class="language-plaintext highlighter-rouge">PyTorch Tensor</code> is basically a generic multidimensional array containing numerical values on which we can perform computational actions.</p>
  </li>
</ul>

<p>In our case, these files contain pytorch objects with textual and tokenized representation of each instruction in your data set:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">{</span>
<span class="s">'instruction'</span><span class="p">:</span> <span class="s">"My instruction..."</span><span class="p">,</span>
<span class="s">'input'</span><span class="p">:</span> <span class="s">'My input...'</span><span class="p">,</span>
<span class="s">'output'</span><span class="p">:</span> <span class="s">"My output..."</span><span class="p">,</span>
<span class="s">'input_ids'</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">19028</span><span class="p">,</span> <span class="mi">304</span><span class="p">,</span> <span class="mi">267</span> <span class="p">,</span> <span class="p">...</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
<span class="s">'input_ids_no_response'</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">19028</span><span class="p">,</span> <span class="mi">304</span><span class="p">,</span> <span class="mi">267</span><span class="p">,</span> <span class="p">...</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
<span class="s">'labels'</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">19028</span><span class="p">,</span> <span class="mi">304</span><span class="p">,</span> <span class="mi">267</span> <span class="p">,</span> <span class="p">...</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="p">}</span></code></pre></figure>

<p>You can see that <code class="language-plaintext highlighter-rouge">input_ids</code> are stored as a tensor, which means we can perform computational actions on it, for example, get the MIN and MAX values of the tensor:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">x</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">19028</span><span class="p">,</span> <span class="mi">304</span><span class="p">,</span> <span class="mi">267</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">aminmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">return_types</span><span class="p">.</span><span class="n">aminmax</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">tensor</span><span class="p">(</span><span class="mi">304</span><span class="p">),</span> <span class="nb">max</span><span class="o">=</span><span class="n">tensor</span><span class="p">(</span><span class="mi">19028</span><span class="p">))</span></code></pre></figure>

<p>There are a lot of out of the box operations we can perform on a tensor, all of them documented in <a href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank">PyTorch’s API</a>.</p>

<h4 id="max-sequence-length">Max sequence length</h4>
<p>Depending on the model capabilities and the hardware you’re running (specifically GPU), you’ll want to set
the max length sequence which will limit the number of tokens produced by the tokenizer to avoid out of memory errors.<br />
As you can see from the example above, the training prompts are tokenized into sequence of numbers representing the text.<br />
The max sequence length parameter limiting the number of produced tokens.</p>

<h3 id="starting-the-actual-fine-tuning">Starting the actual fine tuning</h3>
<p>Once we got the data ready, we can start with the actual fine tuning process by running:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">python</span> <span class="n">finetune</span><span class="o">/</span><span class="n">adapter_v2</span><span class="p">.</span><span class="n">py</span></code></pre></figure>

<p>But before doing so, let’s check out this file and understand how fine tuning works and the configuration we need to apply for our use case.</p>

<h4 id="fine-tuning-flow">Fine-tuning flow</h4>
<p>In our case (and many others), in a high level, fine tuning flow is as follows:</p>

<ul>
  <li>Taking data from the training data set.</li>
  <li>Running X training iterations adjusting the weights and biases.</li>
  <li>Saving checkpoints with weights and biases every Y iterations.</li>
  <li>Evaluating every Z times the performance with a <code class="language-plaintext highlighter-rouge">loss function</code> (explained above).</li>
  <li>If pleased with the results, saves final version of the weights and biases.</li>
</ul>

<p>As you can see from examining the file, we have bunch parameters related to actual flow.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">eval_iters</code> - controls how many times time we will adjust the weights and biases.</li>
  <li><code class="language-plaintext highlighter-rouge">eval_interval</code> - controls how often we will run a loss function to evaluate the performance.</li>
  <li><code class="language-plaintext highlighter-rouge">save_interval</code> - controls how ofter we will save checkpoints during the training.</li>
  <li><code class="language-plaintext highlighter-rouge">devices</code> - determines on how many GPUs we will be running.</li>
</ul>

<p>Now that we got the gist of the fine tuning flow, let’s go over the more significant hyperparameters that 
will determine the quality of the training.</p>

<h4 id="learning-rate-and-catastrophic-forgetting">Learning rate and catastrophic forgetting</h4>
<p>Learning rate is a hyperparameter which dictates how drastically the model weights will be modified during fine tuning.
This should be a relatively a low number (hence the default value is 0.009) so the model will not forget it’s initial training (a process called catastrophic forgetting),
but high enough for the weights to represent the newly learned data.</p>

<h4 id="epochs-and-epoch-sizes">Epochs and epoch sizes</h4>
<p>An <code class="language-plaintext highlighter-rouge">epoch</code> is a term that refers to passing an entire training data set 1 time during fine tuning.<br />
An <code class="language-plaintext highlighter-rouge">epoch_size</code> is the number of total samples to train on.</p>

<h4 id="gradient-accumulation-batches-and-micro-batches">Gradient accumulation, batches and micro-batches</h4>
<p>A <code class="language-plaintext highlighter-rouge">batch</code> is the number of samples to use at a time before updating the weights.<br />
For example, if your entire training dataset consist of 1000 samples, and you define a batch as 100,
it will take 10 iterations to complete 1 epoch.</p>

<p>Finding the optimal batch size to achieve good results with minimum iterations and weights updates is no easy task.
Too big of a batch - you’ll be out of memory,
too small - training will be slow and weight updates will be noisy.</p>

<p>This is where the gradient accumulation technique is used.<br />
Perhaps I’m over simplifying the explanation since this is not a post for data scientists, but in simple terms -
when we’re using gradient accumulation, we are processing <code class="language-plaintext highlighter-rouge">micro_batch</code> size of samples at a given time, which prevents out of memory errors,
accumulating the required parameter changes to minimize the loss (aka make the prediction better), and once the micro batches reached
the batch size, we’re actually updating the parameters (weights and biases).</p>

<p>That way, we’re not constantly updating the parameters which reduces noice and improves performance, nor we’re risking running out of memory.</p>

<p>For clarity, I’ve extracted the relevant code from the fine tuning script into a simple implementation example:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># setting parameters for batch and micro batch
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">/</span> <span class="n">devices</span>
<span class="n">micro_batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">gradient_accumulation_iters</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="n">micro_batch_size</span>

<span class="c1"># iter_num is the number of iteration, omitted from the example
</span><span class="n">is_accumulating</span> <span class="o">=</span> <span class="n">iter_num</span> <span class="o">%</span> <span class="n">gradient_accumulation_iters</span> <span class="o">!=</span> <span class="mi">0</span>

<span class="c1"># accumulate the changes to the parameters
</span><span class="k">with</span> <span class="n">fabric</span><span class="p">.</span><span class="n">no_backward_sync</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="n">is_accumulating</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">lm_head_chunk_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
    <span class="c1"># shift the targets such that output n predicts token n+1
</span>    <span class="n">logits</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][...,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">chunked_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">:])</span>
    <span class="n">fabric</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span> <span class="o">/</span> <span class="n">gradient_accumulation_iters</span><span class="p">)</span>

<span class="c1"># if done accumulating, save
</span><span class="k">if</span> <span class="ow">not</span> <span class="n">is_accumulating</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">step_count</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure>

<h4 id="overfitting-underfitting-and-weight-decay">Overfitting, underfitting and weight decay</h4>
<p>In basic terms,
<code class="language-plaintext highlighter-rouge">overfitting</code> occurs when a model performs very good on data it has seen but very poorly on new unseen data.<br />
<code class="language-plaintext highlighter-rouge">Underfitting</code> occurs when a model performs poorly on both seen and unseen data.<br />
<code class="language-plaintext highlighter-rouge">Weight decay</code> is a technique used to prevent overfitting of a model.
This parameter basically reduce weight adjustment during an update and needs to be carefully adjusted since a number too small
will cause overfitting while a number too big will cause underfitting.</p>

<h4 id="warmup-iterations">Warmup iterations</h4>
<p><code class="language-plaintext highlighter-rouge">Warmup iterations</code> is the number of epoch iterations where the defined <code class="language-plaintext highlighter-rouge">learning rate</code> (aka how drastically weights are changed) is lower than the default learning rate defined for the fine tuning process.<br />
Each warmup iteration the learning rate is increasing up until it reaches the defined hyperparameter value.<br />
It is used to prevent the model’s parameters drastically change upon exposure to new data, which might cause early over fitting and other undesired results.</p>

<h3 id="running-inference">Running inference</h3>
<p>Now that we have finished fine tuning the model we can run an inference.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">## [adapter_path] - Path to the checkpoint with trained adapter weights, which are the output of `adapter_v2.py`.
## [checkpoint_dir] - The path to the checkpoint folder of the base model.
</span><span class="n">python</span> <span class="n">generate</span><span class="o">/</span><span class="n">adapter_v2</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">adapter_path</span> <span class="p">[</span><span class="n">adapter_path</span><span class="p">]</span> <span class="o">--</span><span class="n">checkpoint_dir</span> <span class="p">[</span><span class="n">checkpoint_dir</span><span class="p">]</span> <span class="o">--</span><span class="n">prompt</span> <span class="s">"Annie, are you ok?"</span></code></pre></figure>]]></content><author><name>Jacob Shapira</name></author><category term="jekyll" /><category term="update" /><category term="LLM/ML/AI" /></entry><entry><title type="html">Reducing resource requirements: Hands-on LLM quantization and post-quantization performance assessment for dummies</title><link href="/jekyll/update/2023/09/18/llm-quantization.html" rel="alternate" type="text/html" title="Reducing resource requirements: Hands-on LLM quantization and post-quantization performance assessment for dummies" /><published>2023-09-18T12:31:29+03:00</published><updated>2023-09-18T12:31:29+03:00</updated><id>/jekyll/update/2023/09/18/llm-quantization</id><content type="html" xml:base="/jekyll/update/2023/09/18/llm-quantization.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#intro" id="markdown-toc-intro">Intro</a></li>
  <li><a href="#post-training-quantization-vs-quantization-aware-training" id="markdown-toc-post-training-quantization-vs-quantization-aware-training">Post training quantization VS quantization aware training</a></li>
  <li><a href="#precision-to-memory-usage-prediction" id="markdown-toc-precision-to-memory-usage-prediction">Precision to memory usage prediction</a></li>
  <li><a href="#rule-of-thumb-accuracy-vs-resource-requirements-balance" id="markdown-toc-rule-of-thumb-accuracy-vs-resource-requirements-balance">Rule of thumb: accuracy VS resource requirements balance</a></li>
  <li><a href="#running-quantized-model-inference" id="markdown-toc-running-quantized-model-inference">Running quantized model inference</a></li>
  <li><a href="#automatic-post-quantization-accuracy-evaluation" id="markdown-toc-automatic-post-quantization-accuracy-evaluation">Automatic post-quantization accuracy evaluation</a>    <ul>
      <li><a href="#pre-and-post-quantization-summarization-quality-example-using-evaluate" id="markdown-toc-pre-and-post-quantization-summarization-quality-example-using-evaluate">Pre and post quantization summarization quality example using evaluate</a></li>
    </ul>
  </li>
</ul>

<h3 id="intro">Intro</h3>
<p>In LLM context, <code class="language-plaintext highlighter-rouge">quantization</code> is a technique used to reduce the precision of the models parameters (weights and biases) in order to reduce
the model’s memory footprint.<br />
While it has a lot of advantages, the main ones are that it allows us to reduce hardware requirements for the model we use (or use a bigger model with the same hardware),
and reduce inference cost (or increase the speed).</p>

<p>Similar to <a href="/jekyll/update/2023/09/18/llm-fine-tuning-hypterparameters-simplified.html" target="_blank">Effective LLM fine-tuning for dummies</a>,
the content of this post is for professionals (such as software engineers and architects) which aren’t specialized in AI and data science domains but required to gain
sufficient knowledge to enable and lead features and projects in these domains, so the content is simplified and covers the main subjects without getting into too many details while maintaining the hands-on approach.</p>

<h3 id="post-training-quantization-vs-quantization-aware-training">Post training quantization VS quantization aware training</h3>
<p>Typically, quantization process can be implemented in two different phases, during training (or fine-tuning), or during inference.<br />
Quantization during training usually referred to as <code class="language-plaintext highlighter-rouge">QAT</code> (quantization aware training), while quantization during inference referred to as <code class="language-plaintext highlighter-rouge">PTQ</code> (post training quantization).<br />
<code class="language-plaintext highlighter-rouge">QAT</code> is a computationally expensive process but might result in a better model accuracy than the cheaper, simpler <code class="language-plaintext highlighter-rouge">PTQ</code>.<br />
With that being said, depending on the data and the precision after the quantization, the reduction in the model accuracy is often negligible.
In this post, we’ll do some hands-on <code class="language-plaintext highlighter-rouge">PTQ</code> quantization.</p>

<h3 id="precision-to-memory-usage-prediction">Precision to memory usage prediction</h3>
<p>As a generic rule of thumb, to predict <em>approximate</em> memory requirement of a specific model, you should multiply the number of parameters
by the precision of the weights.<br />
Let’s take <code class="language-plaintext highlighter-rouge">falcon-7b</code> with a <code class="language-plaintext highlighter-rouge">single precision float 32bit</code> weights as an example.<br />
<code class="language-plaintext highlighter-rouge">Float 32bit</code> takes up 4 bytes, so if we multiply the number of the bytes by the number of parameters, we’ll get:<br />
<code class="language-plaintext highlighter-rouge">4 × 7e9 = 2.8e10 = 28,000,000,000 bytes = 28gb</code>.<br />
Based on the same logic, weight reduction to a <code class="language-plaintext highlighter-rouge">half precision float 16bit</code> which takes 2 bytes will require approximately 14gb memory.</p>

<h3 id="rule-of-thumb-accuracy-vs-resource-requirements-balance">Rule of thumb: accuracy VS resource requirements balance</h3>
<p>The optimal weight precision to accuracy balance, like a lot of things in the LLM ecosystem - depends on the use case and the data.<br />
As a rule of thumb, quantizing to <code class="language-plaintext highlighter-rouge">int 8bit</code> offers hardly noticed accuracy degradation while reducing memory requirements by 50% or 75%, depending on the original
parameter precision.<br />
With that being said, more aggressive precision reduction such as <code class="language-plaintext highlighter-rouge">int4</code>, <code class="language-plaintext highlighter-rouge">float4</code>, or even <code class="language-plaintext highlighter-rouge">double quantization</code> does exist and definitely worth testing whether they are good enough
for your data and use case.</p>

<h3 id="running-quantized-model-inference">Running quantized model inference</h3>
<p>Similar to the <a href="/jekyll/update/2023/09/18/llm-fine-tuning-hypterparameters-simplified.html" target="_blank">Effective LLM fine-tuning for dummies</a> post,
I will use <code class="language-plaintext highlighter-rouge">LIT-GPT</code> as the tool due to its simplicity.<br />
Running quantized inference with LIT-GPT is pretty straightforward, for example:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">python</span> <span class="n">generate</span><span class="o">/</span><span class="n">base</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">prompt</span> <span class="s">"Annie, are you ok?"</span> <span class="o">--</span><span class="n">checkpoint_dir</span> <span class="n">checkpoints</span><span class="o">/</span><span class="n">tiiuae</span><span class="o">/</span><span class="n">falcon</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span> <span class="o">--</span><span class="n">precision</span> <span class="n">bf16</span><span class="o">-</span><span class="n">true</span></code></pre></figure>

<p>This command will run an inference with a 16bit precision parameters.<br />
As explained above, there additional options such as <code class="language-plaintext highlighter-rouge">int4</code>, <code class="language-plaintext highlighter-rouge">float4</code> etc.<br />
For the full list of options check <a href="https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/quantize.md" target="_blank">LIT-GPT quantization Github page</a>.</p>

<h3 id="automatic-post-quantization-accuracy-evaluation">Automatic post-quantization accuracy evaluation</h3>
<p>It’s a good idea to manually evaluate handpicked set of examples before and after quantization to assess the model’s performance after quantization.<br />
But if we want to perform a more thorough assessment on a larger test set, or to run it as a step in one of our CI/CD pipelines, an automatic tool will be required.<br />
One of these tools is <code class="language-plaintext highlighter-rouge">evalute</code>, which can be found in <a href="https://huggingface.co/evaluate-metric" target="_blank">hugging face</a> (or <a href="https://github.com/huggingface/evaluate" target="_blank">Github</a>).<br />
<code class="language-plaintext highlighter-rouge">Evaluate</code> provides 53 easy tools evaluation tools which we can leverage to test post quantization model performance.</p>

<h4 id="pre-and-post-quantization-summarization-quality-example-using-evaluate">Pre and post quantization summarization quality example using evaluate</h4>
<p>For the sake of the example, let’s assume that our use case is to test text summarization capabilities of a language model pre- and post-quantization.<br />
<em>One of the multiple</em> metrics we can use to compare the quality of the summary is ROUGE (Recall-Oriented Understudy for Gisting Evaluation).<br />
We can run the evaluation module twice, first time with the pre-quantized inference output, second time with post-quantized - and compare the scores.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">evaluate</span>

<span class="n">rouge</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'rouge'</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Star Trek and Star Wars, while both set in space, differ significantly in their themes and focus. Star Trek, primarily a TV franchise, explores scientific and philosophical issues, emphasizing diplomacy and exploration. Its stories are set in a universe where Earth is part of a vast, multi-species United Federation of Planets. The narrative often revolves around the crew of starships, like the USS Enterprise, as they embark on peaceful explorations and diplomatic missions. In contrast, Star Wars is a space opera centered around the epic battle between good (the Jedi) and evil (the Sith). It's a film series set in a galaxy far, far away, where mystical Force users and grand space battles take center stage. Star Wars is known for its iconic characters like Darth Vader and its focus on destiny and heroism, differing from Star Trek's more grounded and intellectual approach."</span><span class="p">]</span>
<span class="n">references</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Star Trek and Star Wars are both popular space franchises, but they have distinct differences. Star Trek, which originated as a television series, is known for its focus on exploration, science, and philosophy. It is set in a future where Earth is part of an interstellar federation, and the narrative usually involves the crew of spacecraft, such as the Enterprise, engaging in exploration and diplomatic efforts. Star Wars, on the other hand, is a cinematic saga that leans more towards the space opera genre, with a clear delineation of good (represented by the Jedi) and evil (embodied by the Sith). It takes place in a distant galaxy and is more centered on action, adventure, and the mystical Force, with legendary figures like Luke Skywalker and Darth Vader. Unlike Star Trek's emphasis on rationality and discovery, Star Wars focuses on the theme of heroism and the classic battle between light and dark."</span><span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">rouge</span><span class="p">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span><span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="p">{</span><span class="s">'rouge1'</span><span class="p">:</span> <span class="mf">0.5570469798657718</span><span class="p">,</span> <span class="s">'rouge2'</span><span class="p">:</span> <span class="mf">0.23648648648648649</span><span class="p">,</span> <span class="s">'rougeL'</span><span class="p">:</span> <span class="mf">0.3691275167785235</span><span class="p">,</span> <span class="s">'rougeLsum'</span><span class="p">:</span> <span class="mf">0.3691275167785235</span><span class="p">}</span></code></pre></figure>

<p>As explained above, similar ROUGE score does not guarantee a good post quantization accuracy since it just measures similar terms and phrases, but it’s enough
to demonstrate the ease of use of the <code class="language-plaintext highlighter-rouge">evaluate</code> package.</p>]]></content><author><name>Jacob Shapira</name></author><category term="jekyll" /><category term="update" /><category term="LLM/ML/AI" /></entry><entry><title type="html">Leverage AWS Direct Connect to use AWS services without exposing your data to the public internet</title><link href="/jekyll/update/2023/03/26/using-aws-without-exposing-data-to-public-internet.html" rel="alternate" type="text/html" title="Leverage AWS Direct Connect to use AWS services without exposing your data to the public internet" /><published>2023-03-26T11:31:29+02:00</published><updated>2023-03-26T11:31:29+02:00</updated><id>/jekyll/update/2023/03/26/using-aws-without-exposing-data-to-public-internet</id><content type="html" xml:base="/jekyll/update/2023/03/26/using-aws-without-exposing-data-to-public-internet.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#intro" id="markdown-toc-intro">Intro</a></li>
  <li><a href="#selecting-a-connection" id="markdown-toc-selecting-a-connection">Selecting a connection</a></li>
  <li><a href="#creating-vifs-and-directing-traffic-to-the-correct" id="markdown-toc-creating-vifs-and-directing-traffic-to-the-correct">Creating VIFs and directing traffic to the correct</a></li>
  <li><a href="#not-only-for-security" id="markdown-toc-not-only-for-security">Not only for security</a></li>
</ul>

<h3 id="intro">Intro</h3>
<p>While there are a lot of tools to secure your data while being transferred over the internet,
the most secured method of them all is not to do it at all.<br />
Obviously this approach limits us in many ways, including (but not limited to) leveraging the power of public clouds.</p>

<p>Luckily, AWS (and GCP, but it’s out of scope for this post) provides us with a way to use its services without transferring the data over the public internet via AWS Direct Connect.</p>

<p>The general idea of Direct Connect is to create physical connections between AWS Backbone network and your data center via AWS Direct connect location partner.</p>

<p><a href="/assets/post-images/2023-03-26-direct-connect/direct-connect-general-arch.JPG" target="_blank"><img src="/assets/post-images/2023-03-26-direct-connect/direct-connect-general-arch.JPG" alt="AWS Direct Connect" /></a></p>

<p>As shown in the diagram, you can physically connect your data center to a router located in the direct connect partner. 
From there, the partner will cross connect your router to AWS router connected to AWS backbone. 
After the physical connections are done, you need to create logical connections via VIFs (Virtual interface) in order to access
AWS public services (such as S3) and your private VPCs or a transit gateway.</p>

<h3 id="selecting-a-connection">Selecting a connection</h3>
<p>You can select 2 types of connections according to your needs and limitations:</p>

<table>
  <thead>
    <tr>
      <th>Connection Type</th>
      <th>Physical network equipment</th>
      <th>Available bandwidths</th>
      <th>Hourly Pricing</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Hosted connection</td>
      <td>Usually belongs to the service provider and managed by it</td>
      <td>Ranging from 50Mbps to 10Gbps</td>
      <td>Higher than a dedicated connection</td>
    </tr>
    <tr>
      <td>Dedicated connection</td>
      <td>Usually your own</td>
      <td>1, 10 and 100Gbps</td>
      <td>Lower than hosted connection</td>
    </tr>
  </tbody>
</table>

<h3 id="creating-vifs-and-directing-traffic-to-the-correct">Creating VIFs and directing traffic to the correct</h3>
<p>You can have 3 types of VIFs: public, private and transit VIFs.<br />
Public VIF for AWS public services such as S3, private VIF for your VPC and transit VIF for a transit gateway.<br />
Next, in order to correctly direct traffic, you should associate each VIF with a VLAN tag, so the traffic can be routed to the
correct routers on AWS.</p>

<p>A VLAN tag is a 4 byte tag added to the ethernet frame in networks configured to use VLANs.</p>

<p>You should use the same VLAN tags on your end, as illustrated below:</p>

<p><a href="/assets/post-images/2023-03-26-direct-connect/direct-connect-with-vlans.JPG" target="_blank"><img src="/assets/post-images/2023-03-26-direct-connect/direct-connect-with-vlans.JPG" alt="AWS Direct Connect" /></a></p>

<p>This is a high level diagram of how things work. 
You can find a neat <a href="https://docs.aws.amazon.com/directconnect/latest/UserGuide/getting_started.html#ConnectionRequest" target="_blank">step-by-step guide in AWS documentation</a>.</p>

<h3 id="not-only-for-security">Not only for security</h3>
<p>Security is not the only benefit of using direct connect. 
You can also get increased bandwidth throughput and a more consistent network performance than what can be achieved with a public internet connection.</p>]]></content><author><name>Jacob Shapira</name></author><category term="jekyll" /><category term="update" /><category term="DataEngineering" /><category term="BackEnd" /><category term="Performance" /><category term="Cloud" /><category term="Architecture" /><category term="Devops" /></entry><entry><title type="html">Reducing I/O latency on EBS restored from snapshots with AWS FSR</title><link href="/jekyll/update/2023/03/26/aws-fast-snapshot-restore.html" rel="alternate" type="text/html" title="Reducing I/O latency on EBS restored from snapshots with AWS FSR" /><published>2023-03-26T11:31:29+02:00</published><updated>2023-03-26T11:31:29+02:00</updated><id>/jekyll/update/2023/03/26/aws-fast-snapshot-restore</id><content type="html" xml:base="/jekyll/update/2023/03/26/aws-fast-snapshot-restore.html"><![CDATA[<p>When creating an EBS snapshot the data is saved to S3.<br />
Snapshots are incremental, meaning that only the blocks that have been modified later than the most recent snapshot - are saved.<br />
For obvious reasons this makes the snapshot faster to save and cheaper to store.</p>

<p>When initiating an EBS volume from a snapshot, the EBS is almost immediately ready for use.<br />
This is a very neat feature which is available due to the asynchronous nature of how storage blocks are loaded from S3 to EBS.</p>

<p>The data is loaded in 2 ways:</p>
<ol>
  <li>Ongoing process in the background</li>
  <li>In case an access attempt is made to a block which isn’t loaded yet, it immediately downloaded from S3.</li>
</ol>

<p>The caveat in this approach is that if we’ll attempt to access a block which is not downloaded yet,
there will be an I/O latency while the data is being downloaded from S3.</p>

<p>To solve this problem, AWS announced a new feature called Fast Snapshot Restore (FSR).<br />
Enabling FSR for a snapshot increases the amount of resources allocated for the download process.</p>

<p>According to the documentation, enabling FSR on a snapshot can speed up the data loading to ~1TB/Hour,
with current cost of 0.75$ per hour.
So for example, initiating EBS from an FSR enabled snapshot of 10TB will take 10 hours and 7.5$.</p>

<p>Some limitations:<br />
You can enable up to 5 FSR enabled snapshots per region and only to snapshots sized 16TiB or less.</p>

<p>A full documentation of FSR can be found <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html" target="_blank">here</a>.</p>]]></content><author><name>Jacob Shapira</name></author><category term="jekyll" /><category term="update" /><category term="DataEngineering" /><category term="BackEnd" /><category term="Performance" /><category term="Cloud" /><category term="Architecture" /></entry><entry><title type="html">Speeding up spark SQL with adaptive query execution</title><link href="/jekyll/update/2023/02/07/spark-adaptive-query-execution.html" rel="alternate" type="text/html" title="Speeding up spark SQL with adaptive query execution" /><published>2023-02-07T11:31:29+02:00</published><updated>2023-02-07T11:31:29+02:00</updated><id>/jekyll/update/2023/02/07/spark-adaptive-query-execution</id><content type="html" xml:base="/jekyll/update/2023/02/07/spark-adaptive-query-execution.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#intro" id="markdown-toc-intro">Intro</a></li>
  <li><a href="#dynamically-switching-join-strategies" id="markdown-toc-dynamically-switching-join-strategies">Dynamically switching join strategies</a></li>
  <li><a href="#dynamically-coalescing-shuffle-partitions" id="markdown-toc-dynamically-coalescing-shuffle-partitions">Dynamically coalescing shuffle partitions</a></li>
  <li><a href="#dynamically-optimizing-skew-joins" id="markdown-toc-dynamically-optimizing-skew-joins">Dynamically optimizing skew joins</a></li>
</ul>

<h3 id="intro">Intro</h3>
<p>Spark version 2.x introduced CBO (cost-based-optimization) framework.<br />
CBO collects and leverages different data statistics (e.g, row count, number of distinct values, etc.) in order to improve
the quality of query execution plans.</p>

<p>However, building a query plan based on static data which isn’t updated in runtime, comes with some drawbacks.<br />
Some examples will be outdated statistics or inaccurate cardinality estimates.<br />
This can lead to suboptimal query plans.<br />
AQE (adaptive query execution) framework attempts to solve these issues by re-optimizing the query plans based on runtime statistics
collected during the execution.</p>

<p>So, what are the main features that AQE framework brings to the table?</p>

<h3 id="dynamically-switching-join-strategies">Dynamically switching join strategies</h3>
<p>Prior to AQE, one of the optimizations done by spark was switching to broadcast join when an
estimated size of one of the sides of the join could fit well into the memory based on a threshold configuration (default ~10mb).<br />
One of the issues with this approach was that spark couldn’t take applied filters into consideration, so if a table couldn’t fit into the threshold before filtering,
spark wouldn’t attempt to broadcast join it even if it could after applying filters.</p>

<p>With AQE, Spark re-plans the join strategy in runtime based on up-to-date join relation size.</p>

<h3 id="dynamically-coalescing-shuffle-partitions">Dynamically coalescing shuffle partitions</h3>
<p>Shuffle is one of key factors in a query performance, and one of the key factors for a performant shuffle is an optimal number of partitions.
What is an optimal number of partitions?
Well, that’s a hard thing to get.</p>
<ol>
  <li>You need to be familiar with the data.</li>
  <li>Even if you familiar with it, we can always have an unexpected skew in production.</li>
  <li>Optimal partition number might change from stage to stage, and so on.</li>
</ol>

<p>Eventually,
If we’ll have too few partitions for the data at hand, we might encounter spills to disk and uneven distribution of work.
If we’ll have too many partitions, we’ll end up with a lot of tasks and network overhead.</p>

<p>AQE Attempts to address these issues by re-optimizing the optimal number of partitions after every stage of the job, aiming for similar size between all the partitions, considering the definition supplied by the
<code class="language-plaintext highlighter-rouge">spark.sql.adaptive.advisoryPartitionSizeInBytes</code> (with some exceptions like <code class="language-plaintext highlighter-rouge">parallelismFirst</code>) parameter.</p>

<p><a href="/assets/post-images/2023-3-21-aqe/coalese.JPG" target="_blank"><img src="/assets/post-images/2023-3-21-aqe/coalese.JPG" alt="Coalesce" /></a></p>

<h3 id="dynamically-optimizing-skew-joins">Dynamically optimizing skew joins</h3>
<p>Data skew is one of the most frequent reasons for performance issues, especially during join.
While prior to AQE we had to always manually mitigate skew joins (repartitioning, salting etc.), AQE obsoletes some of this work.<br />
AQE optimization detects skews based on shuffle file statistics and automatically splits large partition into smaller sub partitions, which will be joined
with the corresponding (after duplication) partition from the other side respectively.</p>

<p>Before AQE optimization:
<a href="/assets/post-images/2023-3-21-aqe/skew1.JPG" target="_blank"><img src="/assets/post-images/2023-3-21-aqe/skew1.JPG" alt="Skew" /></a></p>

<p>In this case, we will have 4 tasks, 1 task per partition.
The longest task (A P1 to B P1) will take 3 minutes while all the others will take approx. 1 minute, resulting in a total execution time of 3 minutes.</p>

<p>After AQE optimization:</p>

<p><a href="/assets/post-images/2023-3-21-aqe/skew2.JPG" target="_blank"><img src="/assets/post-images/2023-3-21-aqe/skew2.JPG" alt="Skew" /></a></p>

<p>AQE Optimization will split ABP1 into 2 different partitions, duplicate BP1 and join between them,
increasing the number of tasks to 5, but reducing allowing a better parallelism, thus reducing the overall execution time by half.</p>]]></content><author><name>Jacob Shapira</name></author><category term="jekyll" /><category term="update" /><category term="DataEngineering" /><category term="BackEnd" /><category term="Performance" /></entry><entry><title type="html">Examining performance related information of your spark application via spark UI</title><link href="/jekyll/update/2022/11/14/performance-related-info-spark-ui.html" rel="alternate" type="text/html" title="Examining performance related information of your spark application via spark UI" /><published>2022-11-14T11:31:29+02:00</published><updated>2022-11-14T11:31:29+02:00</updated><id>/jekyll/update/2022/11/14/performance-related-info-spark-ui</id><content type="html" xml:base="/jekyll/update/2022/11/14/performance-related-info-spark-ui.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#intro" id="markdown-toc-intro">Intro</a></li>
  <li><a href="#application-event-timeline" id="markdown-toc-application-event-timeline">Application event timeline</a></li>
  <li><a href="#jobs" id="markdown-toc-jobs">Jobs</a></li>
  <li><a href="#job-details" id="markdown-toc-job-details">Job details</a>    <ul>
      <li><a href="#associated-sql-query" id="markdown-toc-associated-sql-query">Associated SQL query</a></li>
      <li><a href="#stages" id="markdown-toc-stages">Stages</a></li>
    </ul>
  </li>
  <li><a href="#stage-details" id="markdown-toc-stage-details">Stage details</a>    <ul>
      <li><a href="#event-timeline" id="markdown-toc-event-timeline">Event timeline</a></li>
      <li><a href="#summary-metrics" id="markdown-toc-summary-metrics">Summary metrics</a></li>
      <li><a href="#tasks-breakdown" id="markdown-toc-tasks-breakdown">Tasks breakdown</a></li>
    </ul>
  </li>
  <li><a href="#the-main-course---understanding-sql-tab" id="markdown-toc-the-main-course---understanding-sql-tab">The main course - Understanding SQL tab</a>    <ul>
      <li><a href="#scanning-parquet" id="markdown-toc-scanning-parquet">Scanning Parquet</a></li>
      <li><a href="#wholestagecodegen---fuse-of-multiple-operators" id="markdown-toc-wholestagecodegen---fuse-of-multiple-operators">WholeStageCodeGen - fuse of multiple operators</a></li>
      <li><a href="#exchange" id="markdown-toc-exchange">Exchange</a></li>
      <li><a href="#narrow-transformation" id="markdown-toc-narrow-transformation">Narrow transformation</a></li>
      <li><a href="#saving-the-output" id="markdown-toc-saving-the-output">Saving the output</a></li>
    </ul>
  </li>
</ul>

<h3 id="intro">Intro</h3>
<p>Debugging and improving spark performance bottlenecks isn’t a straightforward task most of the time.
Performance issues can arise from easy to detect bottlenecks such as wasteful UDFs or issues in the executors,
or a more illusive problems such as estimated metrics of input size in SQL plan being very inaccurate.</p>

<p>Luckily, we have spark UI to aid us, but one must know where to look and how to look at the generic data
provided by spark UI to find potential issues in his spark application.</p>

<p>In this post I’ll try to point to the parts of spark UI which might display information specifically relevant
for various performance issue we might encounter in a spark application.</p>

<p>I always prefer to use examples, so for this post I’ve written a simple application which performs tasks we’ll usually
encounter in spark applications, and <em>deliberately</em> causes some common performances issues.</p>

<p>This app will be:</p>

<ul>
  <li>Reading Data</li>
  <li>Doing Wide Transformation</li>
  <li>Doing Narrow Transformation</li>
  <li>Causing shuffle read and writes</li>
  <li>Causing data spill</li>
  <li>Causing unevenly distributed workload</li>
  <li>Saving Data</li>
</ul>

<p>I’ve written it in pyspark, but the idea would’ve remained the same even if it was written in scala or java.</p>

<p>We have 2 parquet files,<br />
<em>cities</em> - Containing skewed and duplicate data about world cities and countries:</p>

<figure class="highlight"><pre><code class="language-xml" data-lang="xml">+--------------------+------+
|             country| count|
+--------------------+------+
|              Russia|210355|
|            Malaysia| 25806|
|              France|   633|
|       United States|531703|
|               China|   799|
|             Nigeria| 45254|
|               Spain|   569|</code></pre></figure>

<p><em>citizens</em> - Containing citizens count per city.</p>

<figure class="highlight"><pre><code class="language-xml" data-lang="xml">|         name|citizens|      country|
......................................
|   New York | 8467513 | United States|
|     Moscow | 1700000  | Russia       |
|     Madrid | 600000 | Spain       |
......................................</code></pre></figure>

<p>Both of the files saved in parquet format and partitioned by the country field.</p>

<p>And we have the following self-explanatory app:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Read files into a dataframe
</span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">upper</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">floor</span><span class="p">,</span> <span class="n">spark_partition_id</span>
<span class="n">cities</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"parquet"</span><span class="p">).</span><span class="n">load</span><span class="p">(</span><span class="s">"/folder/cities"</span><span class="p">)</span>
<span class="n">citizens</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"parquet"</span><span class="p">).</span><span class="n">load</span><span class="p">(</span><span class="s">"/folder/citizens"</span><span class="p">)</span>

<span class="c1"># Join cities info with citizens info
</span><span class="n">cities_full_info</span> <span class="o">=</span> <span class="n">cities</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">citizens</span><span class="p">,</span> <span class="p">[</span><span class="s">"name"</span><span class="p">,</span> <span class="s">"country"</span><span class="p">])</span>

<span class="c1"># Repartition into too few partitions
</span><span class="n">cities_full_info</span> <span class="o">=</span> <span class="n">cities_full_info</span><span class="p">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Add citizens growth estimation for next year
</span><span class="n">cities_full_info</span> <span class="o">=</span> <span class="n">cities_full_info</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">'citizens_next_year'</span><span class="p">,</span> <span class="n">floor</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"citizens"</span><span class="p">)</span><span class="o">*</span><span class="mf">1.1</span><span class="p">))</span>

<span class="c1"># Write the joined data back to parquet
</span><span class="n">cities_full_info</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"parquet"</span><span class="p">).</span><span class="n">mode</span><span class="p">(</span><span class="s">"overwrite"</span><span class="p">).</span><span class="n">save</span><span class="p">(</span><span class="s">"cities_full_info"</span><span class="p">)</span></code></pre></figure>

<p>Let’s assume our real production app is not as simple as the example, and no one provided us with a list of issues it will cause.
The only thing we know is that it runs slow, and we need to debug it.
What are the steps, and at which specific data in spark UI should we look to detect potential performance bottlenecks?</p>

<h3 id="application-event-timeline">Application event timeline</h3>
<p>First, make sure all the executors you expect to participate are added to the application.
If not, it might indicate an infrastructure issue, meaning you’re running on less resources than expected.</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/event-timeline.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/event-timeline.JPG" alt="Event Timeline" /></a></p>

<h3 id="jobs">Jobs</h3>
<p>Next, it might be a good idea to get a high level understanding of costly jobs in your app.
You can scroll down to completed (or running) jobs and checkout the <code class="language-plaintext highlighter-rouge">duration</code> column to understand which jobs taking longer than expected.</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/completed-jobs.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/completed-jobs.JPG" alt="Jobs" /></a></p>

<h3 id="job-details">Job details</h3>
<p>In the job details page, we get some useful information.</p>

<h4 id="associated-sql-query">Associated SQL query</h4>
<p>We will examine SQL tab later, but if you were using Datasets or DataFrames (which you probably are), you can examine the physical and
logical plans of the SQL query by clicking on the associated query ID.</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/assosiated-query.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/assosiated-query.JPG" alt="AssociatedSQL" /></a></p>

<p>We will get into the details of this later.</p>

<h4 id="stages">Stages</h4>
<p>Each job has 1 or more stages, and in the stage summary you can see which one of the stages might be a bottleneck.
In our example we have only 1 stage, so we will explore it:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/stages.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/stages.JPG" alt="Stages" /></a></p>

<h3 id="stage-details">Stage details</h3>
<p>In stage details we can see aggregated and per-executor metrics of the stage.</p>

<h4 id="event-timeline">Event timeline</h4>
<p>An important thing we might want to examine is that the workload is distributed evenly among the workers.
Which in our case we can see is not the case in this particular stage:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/bad-worker-distribution.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/bad-worker-distribution.JPG" alt="Bad Worker distribution" /></a></p>

<p>We can compare it to a “healthy” work distribution in the read phase of our app, which looks like this:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/worker-distribution.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/worker-distribution.JPG" alt="Healthy Worker distribution" /></a></p>

<h4 id="summary-metrics">Summary metrics</h4>
<p>In the summary metrics you can see summarized stats of all the tasks, but two indicators are specifically important.
<code class="language-plaintext highlighter-rouge">Data spills</code> and <code class="language-plaintext highlighter-rouge">Shuffle read or write</code>.
These are extremely costly operations, and we need to understand whether we can improve or get rid of it all together.</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/summary-metrics.jpg" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/summary-metrics.jpg" alt="Summary Metrics" /></a></p>

<h4 id="tasks-breakdown">Tasks breakdown</h4>
<p>In the task breakdown, we may examine metrics per task and per executor to get a clearer picture.
But we also have additional information which might be useful for understanding potential slowness - that is the <code class="language-plaintext highlighter-rouge">data locality level</code>,
data locality indicates data access latency.
Possible values will be:</p>

<ul>
  <li>PROCESS_LOCAL - data co-located with the code in the same JVM</li>
  <li>NODE_LOCAL - data located on the same node</li>
  <li>NO_PREF - data with no preference for locality</li>
  <li>RACK_LOCAL - data on the same rack but on a different server</li>
  <li>ANY - Data located on other racks</li>
</ul>

<p><a href="/assets/post-images/2022-11-15-spark-ui/data-per-task-per-executor.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/data-per-task-per-executor.JPG" alt="Metrics per task, per executor" /></a></p>

<h3 id="the-main-course---understanding-sql-tab">The main course - Understanding SQL tab</h3>
<p>In my opinion this is the most important part to understand since it gives us a clear picture of the application flow,
how much data was moved, how it was moved, which functionality was executed and how long any of them took.
Before examining the logical and physical plan of our application, make sure to toggle on the
<code class="language-plaintext highlighter-rouge">Show the Stage ID and Task ID that corresponds to the max metric</code> checkbox, this will help us to correlate parts of the plan
to bottlenecks we saw in the jobs and stages details.</p>

<h4 id="scanning-parquet">Scanning Parquet</h4>

<p><a href="/assets/post-images/2022-11-15-spark-ui/sql-1.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/sql-1.JPG" alt="Scan Parquet" /></a></p>

<p>First, we can see the <code class="language-plaintext highlighter-rouge">Scan parquet</code> stage which show information about our file structure in HDFS and how long it took the application to
obtain it (Listing leaf and files job).
We can see the number of partitions and files read, both stating 244 - which makes sense since our data was partitioned by country
with a single parquet file per partition:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/hue.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/hue.JPG" alt="Hue" /></a></p>

<p>We can see how long it took to complete the scan (1.2s, 5.6s) and number of output rows.
If we hover over this stage we can see additional info such as the directory, schema, and filters that were pushed:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/sql-1-1.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/sql-1-1.JPG" alt="Directory and schema" /></a></p>

<h4 id="wholestagecodegen---fuse-of-multiple-operators">WholeStageCodeGen - fuse of multiple operators</h4>
<p><a href="/assets/post-images/2022-11-15-spark-ui/sql-2.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/sql-2.JPG" alt="WholeStageCodeGen" /></a></p>

<p>WholeStageCodegen fuses multiple operators together into a single Java function that is aimed at improving execution performance.
It collapses a query into a single optimized function that eliminates virtual function calls and leverages CPU registers for intermediate data.
In our case, we can see that fuse of 3 different functions into a single WholeStageCodegen:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">ColumnarToRow</code> - which generates spark data frame from parquet</li>
  <li><code class="language-plaintext highlighter-rouge">Filter</code> - which was pushed on the <code class="language-plaintext highlighter-rouge">name</code> column by spark due to the join that will come next</li>
  <li><code class="language-plaintext highlighter-rouge">BroadcatHashJoin</code> - which joins <code class="language-plaintext highlighter-rouge">citizens</code> and <code class="language-plaintext highlighter-rouge">cities</code> data together (via broadcast due to the relatively small data size)</li>
</ol>

<p>In both <code class="language-plaintext highlighter-rouge">Filter</code> and <code class="language-plaintext highlighter-rouge">BroadcastHashJoin</code> we can conveniently see the number of output rows after a specific operator applied.</p>

<p>Next, we will see the <code class="language-plaintext highlighter-rouge">Project</code> operator, which simply represents what columns will be selected.
In our case, we will see the product of the join between the 2 dataframes:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/product-of-join.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/product-of-join.JPG" alt="Project" /></a></p>

<h4 id="exchange">Exchange</h4>
<p>Next, we will see an <code class="language-plaintext highlighter-rouge">exchange</code> in the plan which was caused by our <code class="language-plaintext highlighter-rouge">repartition</code> command.
<a href="/assets/post-images/2022-11-15-spark-ui/exchange.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/exchange.JPG" alt="Exchange" /></a></p>

<p>If we hover over the box, we can see the partition method that was used (round-robin).
We can also see the total data size, number of partitions, records and various other metrics.</p>

<p>An important thing to notice here (and in the previous step of <code class="language-plaintext highlighter-rouge">BroadcastHashJoin</code>) is the number of output rows of the join.
If the number of the output rows is disproportional or doesn’t make sense from your understanding of the data, it might indicate that
there’s something wrong with your data, such as unexpected duplicates in the join column, or your assumptions about the data are incorrect.</p>

<h4 id="narrow-transformation">Narrow transformation</h4>
<p><a href="/assets/post-images/2022-11-15-spark-ui/next_year_and_write.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/next_year_and_write.JPG" alt="Exchange" /></a></p>

<p>Here you can see a <code class="language-plaintext highlighter-rouge">WholeStageCodegen</code> generated for our citizen multiplication command.</p>

<p>Please note that the duration we see in the WholeStageCodegen (1.3m, see image below) is combining the execution of the repartition command <em>and</em> the column value manipulation.
If we were to remove the repartition command, the column value manipulation would’ve moved to the top <code class="language-plaintext highlighter-rouge">WholeStageCodegen</code> function alongside the filter and the broadcast join.</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/narrow-duration.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/narrow-duration.JPG" alt="Exchange" /></a></p>

<h4 id="saving-the-output">Saving the output</h4>
<p><a href="/assets/post-images/2022-11-15-spark-ui/writing.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/writing.JPG" alt="Exchange" /></a></p>

<p>In the end, we see <code class="language-plaintext highlighter-rouge">InsertIntoHadoopFsRelationCommand</code>, which representing the writing of the output back to parquet.</p>

<p>Among a lot of interesting metrics, there are few which are performance significant:</p>
<ol>
  <li>Number of output rows &amp; data size - this is kind of self-explanatory.</li>
  <li>Number of written files - this is important to notice, since constantly creating a lot of small files might impact the performance of whatever reads your data. A good file size will be 512MiB or 1GiB.</li>
</ol>]]></content><author><name>Jacob Shapira</name></author><category term="jekyll" /><category term="update" /><category term="DataEngineering" /><category term="BackEnd" /><category term="Performance" /></entry><entry><title type="html">Improve MTTR’s with observability platforms</title><link href="/jekyll/update/2022/10/23/improve-mttr-with-observability-platform.html" rel="alternate" type="text/html" title="Improve MTTR’s with observability platforms" /><published>2022-10-23T19:31:29+03:00</published><updated>2022-10-23T19:31:29+03:00</updated><id>/jekyll/update/2022/10/23/improve-mttr-with-observability-platform</id><content type="html" xml:base="/jekyll/update/2022/10/23/improve-mttr-with-observability-platform.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#intro" id="markdown-toc-intro">Intro</a></li>
  <li><a href="#observability-platforms-to-reduce-the-pain" id="markdown-toc-observability-platforms-to-reduce-the-pain">Observability platforms to reduce the pain</a></li>
  <li><a href="#pricing" id="markdown-toc-pricing">Pricing</a></li>
  <li><a href="#demos--choosing-the-right-tool-for-you" id="markdown-toc-demos--choosing-the-right-tool-for-you">Demos &amp; Choosing the right tool for you</a></li>
</ul>

<h3 id="intro">Intro</h3>
<p>Ability to quickly resolve production issues is one of the key factors in customer satisfaction.<br />
Not the mention it saves us resources and debugging time.<br />
Cloud native and distributed systems are especially hard to debug due to high amount of moving parts.<br />
An error in a flow can originate in a one of N containers of a specific service, in a flow composed of N different services, 
and the erroneous container might as well be dead already due to different reasons (HPA, a crash, etc’).<br />
Debugging is hard, reproducing is sometimes harder.</p>

<h3 id="observability-platforms-to-reduce-the-pain">Observability platforms to reduce the pain</h3>
<p>Tools such as Rookout, Lightrun or Helios can greatly reduce debugging complexity and MTTR’s (Mean time to response/repair/resolve/recover).</p>

<p>For example,
You can set ad-hoc virtual non-breaking breakpoints in a live production environment.
You’ll be able to get information (such as variables, context, evaluate expressions and basically almost everything you’ll get from a debugger) from any service, down to specific line of code, if it was triggered during a flow.
Thus making debugging much easier.</p>

<p>You can also get live visual representation of how components in your flow interconnects with each other, down to API payloads, kafka messages and more.
You can even reproduce entire flows with a click of a button.</p>

<p>Basically, you take the power and flexibility of local debugging and issue investigation and apply it on a production (or test/staging) environment.</p>

<h3 id="pricing">Pricing</h3>
<p>The pricing of most of these tools a relatively affordable for most companies.
Obviously the exact price depending on the tool itself and the package you select, but in general it starts at around 700$/mo at the time of writing this post.</p>

<h3 id="demos--choosing-the-right-tool-for-you">Demos &amp; Choosing the right tool for you</h3>
<p>You can book an official demo in the websites, but if you’d like a quick overview here are some useful youtube videos:</p>
<ul>
  <li><a href="https://www.youtube.com/watch?v=5hR17z4Qm3g" target="_blank">Lightrun</a></li>
  <li><a href="https://www.youtube.com/watch?v=Tnsd_65jQLY" target="_blank">Rookout</a></li>
  <li><a href="https://www.youtube.com/watch?v=OI5oYxTCiV0&amp;t=957s" target="_blank">Helios</a></li>
</ul>]]></content><author><name>Jacob Shapira</name></author><category term="jekyll" /><category term="update" /><category term="Architecture" /><category term="Cloud" /><category term="BackEnd" /><category term="Devops" /></entry><entry><title type="html">Extending Flyweight pattern ideas to improve system-wide performance and reduce costs</title><link href="/jekyll/update/2022/07/10/leveraging-flyweight-pattern-to-system-arch.html" rel="alternate" type="text/html" title="Extending Flyweight pattern ideas to improve system-wide performance and reduce costs" /><published>2022-07-10T19:31:29+03:00</published><updated>2022-07-10T19:31:29+03:00</updated><id>/jekyll/update/2022/07/10/leveraging-flyweight-pattern-to-system-arch</id><content type="html" xml:base="/jekyll/update/2022/07/10/leveraging-flyweight-pattern-to-system-arch.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#intro" id="markdown-toc-intro">Intro</a>    <ul>
      <li><a href="#taking-the-general-idea-to-a-system-wide-level" id="markdown-toc-taking-the-general-idea-to-a-system-wide-level">Taking the general idea to a system-wide level</a></li>
      <li><a href="#be-careful-with-the-tradeoffs" id="markdown-toc-be-careful-with-the-tradeoffs">Be careful with the tradeoffs</a></li>
    </ul>
  </li>
</ul>

<h3 id="intro">Intro</h3>
<p>There are plenty of sources online explaining the Flyweight design pattern in depth.<br />
The most common description of the pattern is that it designed to reduce memory footprint of a program without losing any capability.
But the ideas behind it can be extended to a broader, system-wide scale.</p>

<p>I won’t get into the depth of the design pattern itself (if you’re not familiar with it, you can check out a <a href="https://en.wikipedia.org/wiki/Flyweight_pattern" target="_blank">good explanation in wikipedia</a>),
but I will sum it up to a very simplistic explanation:</p>

<p>The Flyweight pattern suggests minimizing the memory footprint of a large amount of objects, by storing commonly shared data in external data structures,
and share these data structures with the objects themselves.</p>

<p>Terminology wise, we’ll have:</p>

<ul>
  <li>Intrinsic state, which refers to the commonly shared data.</li>
  <li>Extrinsic state, which refers to context related data, we can call it <i>dynamic</i> data.</li>
</ul>

<p>A simple example for it can be bullets in a shooter game,
an extrinsic state will refer to the x,y of the bullet location, while the intrinsic state will refer to the color, size and other constant properties of the bullet.
We will have a lot of minimalistic bullet objects, which will refer to a single graphical representation object.</p>

<h4 id="taking-the-general-idea-to-a-system-wide-level">Taking the general idea to a system-wide level</h4>
<p>Let’s say you’re working on a cyber-security system which monitors and ingests network traffic from 3rd party sensors, analyzes the traffic and reports suspicious activity.
It’s simplified architecture will look something like this:</p>

<p><a href="/assets/post-images/2022-08-10-flyweight/simplified-event-ingestion.JPG" target="_blank"><img src="/assets/post-images/2022-08-10-flyweight/simplified-event-ingestion.JPG" alt="Simplified Ingesting flow" /></a></p>

<p>Let’s assume that one of the events the sensor records, is DNS lookups done by endpoints and servers in the network.
The sensor enriches it with owner info - such as ASN, and produce it to Kafka, from which ETL reads the event and saves it to storage.
The event structured something like this:</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="w">
  </span><span class="nl">"event_initiator_ip"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1.1.1.1"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dns_lookup"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"domain_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"www.google.com"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"record_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"a"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"ttl"</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w">
  </span><span class="nl">"address"</span><span class="p">:</span><span class="w"> </span><span class="s2">"142.251.40.164"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"owner_asn_info"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"ASNumber"</span><span class="p">:</span><span class="w"> </span><span class="s2">"15169"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"ASName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GOOGLE"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"RegDate"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2000-03-30"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"Updated"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2012-02-24"</span><span class="p">,</span><span class="w"> 
    </span><span class="nl">"Ref"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://rdap.arin.net/registry/autnum/15169"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"OrgName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Google LLC"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"OrgId"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GOGL"</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"owner_whois_info"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"NetRange"</span><span class="p">:</span><span class="w">       </span><span class="s2">"142.250.0.0 - 142.251.255.255"</span><span class="w">
    </span><span class="nl">"CIDR"</span><span class="p">:</span><span class="w">           </span><span class="s2">"142.250.0.0/15"</span><span class="w">
    </span><span class="nl">"NetName"</span><span class="p">:</span><span class="w">        </span><span class="s2">"GOOGLE"</span><span class="w">
    </span><span class="nl">"NetHandle"</span><span class="p">:</span><span class="w">      </span><span class="s2">"NET-142-250-0-0-1"</span><span class="w">
    </span><span class="nl">"Parent"</span><span class="p">:</span><span class="w">         </span><span class="s2">"NET142 (NET-142-0-0-0-0)"</span><span class="w">
    </span><span class="nl">"NetType"</span><span class="p">:</span><span class="w">        </span><span class="s2">"Direct Allocation"</span><span class="w">
    </span><span class="nl">"OriginAS"</span><span class="p">:</span><span class="w">       </span><span class="s2">"AS15169"</span><span class="w">
    </span><span class="nl">"Organization"</span><span class="p">:</span><span class="w">   </span><span class="s2">"Google LLC (GOGL)"</span><span class="w">
    </span><span class="nl">"RegDate"</span><span class="p">:</span><span class="w">        </span><span class="s2">"2012-05-24"</span><span class="w">
    </span><span class="nl">"Updated"</span><span class="p">:</span><span class="w">        </span><span class="s2">"2012-05-24"</span><span class="w">
    </span><span class="nl">"Ref"</span><span class="p">:</span><span class="w">            </span><span class="s2">"https://rdap.arin.net/registry/ip/142.250.0.0"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<p>For simplicity of the example, lets assume we’re not compressing, nor using better suited formats to store the data,
and we’re just saving it as is in JSON format. This will cost us roughly 1kb per event.
In a network with 10K/S lookups to google, it means a rough estimate of 864gb of daily data only for DNS lookups.</p>

<ul>
  <li>It means 864gb worth of daily addition to storage</li>
  <li>It means that if we’ll attempt to write a daily scheduled analytic, it’ll have to handle 864gb of data</li>
</ul>

<p>You probably already see where I’m going with this.</p>

<p>If we take a close look at the event data, we’ll see that the owner info is not context related, and probably won’t be frequently modified
between each lookup event for the same domain. We can consider it as an intrinsic state.</p>

<p>If we change our ETL to work with a cache of ownership info, writing it to a separate intrinsic data storage only when needed,
we can reduce the amount of data we need to store and process by roughly 84%.</p>

<p>Our updated architecture will look something like this:</p>

<p><a href="/assets/post-images/2022-08-10-flyweight/intrinsic-extrinsic-storage-arch.JPG" target="_blank"><img src="/assets/post-images/2022-08-10-flyweight/intrinsic-extrinsic-storage-arch.JPG" alt="Updated Architecture" /></a></p>

<p>The majority of the data (extrinsic data) will be reduced to this:</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="w">
  </span><span class="nl">"event_initiator_ip"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1.1.1.1"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dns_lookup"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"domain_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"www.google.com"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"record_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"a"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"ttl"</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w">
  </span><span class="nl">"address"</span><span class="p">:</span><span class="w"> </span><span class="s2">"142.251.40.164"</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<h4 id="be-careful-with-the-tradeoffs">Be careful with the tradeoffs</h4>
<p>As you probably understand, separating the data to intrinsic and extrinsic states is not a cost free action.
You need to consider your data and it’s usages before separating it.</p>

<p>In our example, the ETL will have to do more work before saving the data:</p>
<ol>
  <li>Managing a cache, in memory or external</li>
  <li>Checking for existence of owner data and comparing the update dates</li>
  <li>Doing an extra store action in case of a new intrinsic info</li>
</ol>

<p>A lot will be decided by the implementation.</p>

<p>For example,
in our use case, we can decide that only common domxzxains (google, stackoverflow etc’) and their owner info will be go through intrinsic pipeline, and use in memory cache,
which will mean two things:</p>

<ol>
  <li>Working with cache will be extremely performant</li>
  <li>Storing most common domains will reduce the majority of the data, while taking relatively small amount of memory</li>
</ol>

<p>Another thing to consider is the usage of the data, the analytics in our case - <br />
can we preload the intrinsic data in the analytics? If not, will JOINing (with spark, for example) of small amount of intrinsic data
will be faster than working with 84% more data which is already in the same dataframes?</p>]]></content><author><name>Jacob Shapira</name></author><category term="jekyll" /><category term="update" /><category term="Architecture" /><category term="DataEngineering" /><category term="BackEnd" /><category term="Performance" /><category term="Cloud" /></entry></feed>
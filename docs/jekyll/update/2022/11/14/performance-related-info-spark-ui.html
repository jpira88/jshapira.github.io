<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Examining performance related information of your spark application via spark UI | Jacob Shapira</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Examining performance related information of your spark application via spark UI" />
<meta name="author" content="Jacob Shapira" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Debugging and improving spark performance bottlenecks isn’t a straightforward task most of the time. Performance issues can arise from easy to detect bottlenecks such as wasteful UDFs or issues in the executors, or a more illusive problems such as estimated metrics of input size in SQL plan being very inaccurate." />
<meta property="og:description" content="Debugging and improving spark performance bottlenecks isn’t a straightforward task most of the time. Performance issues can arise from easy to detect bottlenecks such as wasteful UDFs or issues in the executors, or a more illusive problems such as estimated metrics of input size in SQL plan being very inaccurate." />
<link rel="canonical" href="https://jpira88.github.io/jacobshapira.github.io/jekyll/update/2022/11/14/performance-related-info-spark-ui.html" />
<meta property="og:url" content="https://jpira88.github.io/jacobshapira.github.io/jekyll/update/2022/11/14/performance-related-info-spark-ui.html" />
<meta property="og:site_name" content="Jacob Shapira" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-11-14T11:31:29+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Examining performance related information of your spark application via spark UI" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Examining performance related information of your spark application via spark UI","dateModified":"2022-11-14T11:31:29+02:00","datePublished":"2022-11-14T11:31:29+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jpira88.github.io/jacobshapira.github.io/jekyll/update/2022/11/14/performance-related-info-spark-ui.html"},"url":"https://jpira88.github.io/jacobshapira.github.io/jekyll/update/2022/11/14/performance-related-info-spark-ui.html","author":{"@type":"Person","name":"Jacob Shapira"},"description":"Debugging and improving spark performance bottlenecks isn’t a straightforward task most of the time. Performance issues can arise from easy to detect bottlenecks such as wasteful UDFs or issues in the executors, or a more illusive problems such as estimated metrics of input size in SQL plan being very inaccurate.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/jacobshapira.github.io/assets/css/styles.css">
  <link rel="stylesheet" href="/jacobshapira.github.io/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://jpira88.github.io/jacobshapira.github.io/feed.xml" title="Jacob Shapira" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/jacobshapira.github.io/">Jacob Shapira</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/jacobshapira.github.io/about/">About</a><a class="page-link" href="/jacobshapira.github.io/tag/%23FE/">#FE</a><a class="page-link" href="/jacobshapira.github.io/tag/%23JS/TS/">#JS/TS</a><a class="page-link" href="/jacobshapira.github.io/tag/%23BE/">#BE</a><a class="page-link" href="/jacobshapira.github.io/tag/%23PHP/">#PHP</a><a class="page-link" href="/jacobshapira.github.io/tag/%23Performance/">#Performance</a><a class="page-link" href="/jacobshapira.github.io/tag/%23DataEngineering/">#DataEngineering</a><a class="page-link" href="/jacobshapira.github.io/tag/%23Java/">#Java</a><a class="page-link" href="/jacobshapira.github.io/tag/%23Python/">#Python</a><a class="page-link" href="/jacobshapira.github.io/tag/%23Architecture/">#Architecture</a><a class="page-link" href="/jacobshapira.github.io/tag/%23Devops/">#Devops</a><a class="page-link" href="/jacobshapira.github.io/tag/%23Cloud/">#Cloud</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Examining performance related information of your spark application via spark UI</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-11-14T11:31:29+02:00" itemprop="datePublished">Nov 14, 2022
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Jacob Shapira</span></span></p>
  </header>

  
  
  
  

  <div class="post-content e-content" itemprop="articleBody">
    <p>Debugging and improving spark performance bottlenecks isn’t a straightforward task most of the time.
Performance issues can arise from easy to detect bottlenecks such as wasteful UDFs or issues in the executors,
or a more illusive problems such as estimated metrics of input size in SQL plan being very inaccurate.</p>

<p>Luckily, we have spark UI to aid us, but one must know where to look and how to look at the generic data
provided by spark UI to find potential issues in his spark application.</p>

<p>In this post I’ll try to point to the parts of spark UI which might display information specifically relevant
for various performance issue we might encounter in a spark application.</p>

<p>I always prefer to use examples, so for this post I’ve written a simple application which performs tasks we’ll usually
encounter in spark applications, and <em>deliberately</em> causes some common performances issues.</p>

<p>This app will be:</p>

<ul>
  <li>Reading Data</li>
  <li>Doing Wide Transformation</li>
  <li>Doing Narrow Transformation</li>
  <li>Causing shuffle read and writes</li>
  <li>Causing data spill</li>
  <li>Causing unevenly distributed workload</li>
  <li>Saving Data</li>
</ul>

<p>I’ve written it in pyspark, but the idea would’ve remained the same even if it was written in scala or java.</p>

<p>We have 2 parquet files,<br />
<em>cities</em> - Containing skewed and duplicate data about world cities and countries:</p>

<figure class="highlight"><pre><code class="language-xml" data-lang="xml">+--------------------+------+
|             country| count|
+--------------------+------+
|              Russia|210355|
|            Malaysia| 25806|
|              France|   633|
|       United States|531703|
|               China|   799|
|             Nigeria| 45254|
|               Spain|   569|</code></pre></figure>

<p><em>citizens</em> - Containing citizens count per city.</p>

<figure class="highlight"><pre><code class="language-xml" data-lang="xml">|         name|citizens|      country|
......................................
|   New York | 8467513 | United States|
|     Moscow | 1700000  | Russia       |
|     Madrid | 600000 | Spain       |
......................................</code></pre></figure>

<p>Both of the files saved in parquet format and partitioned by the country field.</p>

<p>And we have the following self-explanatory app:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Read files into a dataframe
</span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">upper</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">floor</span><span class="p">,</span> <span class="n">spark_partition_id</span>
<span class="n">cities</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"parquet"</span><span class="p">).</span><span class="n">load</span><span class="p">(</span><span class="s">"/folder/cities"</span><span class="p">)</span>
<span class="n">citizens</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"parquet"</span><span class="p">).</span><span class="n">load</span><span class="p">(</span><span class="s">"/folder/citizens"</span><span class="p">)</span>

<span class="c1"># Join cities info with citizens info
</span><span class="n">cities_full_info</span> <span class="o">=</span> <span class="n">cities</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">citizens</span><span class="p">,</span> <span class="p">[</span><span class="s">"name"</span><span class="p">,</span> <span class="s">"country"</span><span class="p">])</span>

<span class="c1"># Repartition into too few partitions
</span><span class="n">cities_full_info</span> <span class="o">=</span> <span class="n">cities_full_info</span><span class="p">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Add citizens growth estimation for next year
</span><span class="n">cities_full_info</span> <span class="o">=</span> <span class="n">cities_full_info</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">'citizens_next_year'</span><span class="p">,</span> <span class="n">floor</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"citizens"</span><span class="p">)</span><span class="o">*</span><span class="mf">1.1</span><span class="p">))</span>

<span class="c1"># Write the joined data back to parquet
</span><span class="n">cities_full_info</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"parquet"</span><span class="p">).</span><span class="n">mode</span><span class="p">(</span><span class="s">"overwrite"</span><span class="p">).</span><span class="n">save</span><span class="p">(</span><span class="s">"cities_full_info"</span><span class="p">)</span></code></pre></figure>

<p>Let’s assume our real production app is not as simple as the example, and no one provided us with a list of issues it will cause.
The only thing we know is that it runs slow, and we need to debug it.
What are the steps, and at which specific data in spark UI should we look to detect potential performance bottlenecks?</p>

<h3 id="application-event-timeline">Application event timeline</h3>
<p>First, make sure all the executors you expect to participate are added to the application.
If not, it might indicate an infrastructure issue, meaning you’re running on less resources than expected.</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/event-timeline.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/event-timeline.JPG" alt="Event Timeline" /></a></p>

<h3 id="jobs">Jobs</h3>
<p>Next, it might be a good idea to get a high level understanding of costly jobs in your app.
You can scroll down to completed (or running) jobs and checkout the <code class="language-plaintext highlighter-rouge">duration</code> column to understand which jobs taking longer than expected.</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/completed-jobs.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/completed-jobs.JPG" alt="Jobs" /></a></p>

<h3 id="job-details">Job details</h3>
<p>In the job details page, we get some useful information.</p>

<h4 id="associated-sql-query">Associated SQL query</h4>
<p>We will examine SQL tab later, but if you were using Datasets or DataFrames (which you probably are), you can examine the physical and
logical plans of the SQL query by clicking on the associated query ID.</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/assosiated-query.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/assosiated-query.JPG" alt="AssociatedSQL" /></a></p>

<p>We will get into the details of this later.</p>

<h4 id="stages">Stages</h4>
<p>Each job has 1 or more stages, and in the stage summary you can see which one of the stages might be a bottleneck.
In our example we have only 1 stage, so we will explore it:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/stages.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/stages.JPG" alt="Stages" /></a></p>

<h3 id="stage-details">Stage details</h3>
<p>In stage details we can see aggregated and per-executor metrics of the stage.</p>

<h4 id="event-timeline">Event timeline</h4>
<p>An important thing we might want to examine is that the workload is distributed evenly among the workers.
Which in our case we can see is not the case in this particular stage:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/bad-worker-distribution.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/bad-worker-distribution.JPG" alt="Bad Worker distribution" /></a></p>

<p>We can compare it to a “healthy” work distribution in the read phase of our app, which looks like this:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/worker-distribution.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/worker-distribution.JPG" alt="Healthy Worker distribution" /></a></p>

<h4 id="summary-metrics">Summary metrics</h4>
<p>In the summary metrics you can see summarized stats of all the tasks, but two indicators are specifically important.
<code class="language-plaintext highlighter-rouge">Data spills</code> and <code class="language-plaintext highlighter-rouge">Shuffle read or write</code>.
These are extremely costly operations, and we need to understand whether we can improve or get rid of it all together.</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/summary-metrics.jpg" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/summary-metrics.jpg" alt="Summary Metrics" /></a></p>

<h4 id="tasks-breakdown">Tasks breakdown</h4>
<p>In the task breakdown, we may examine metrics per task and per executor to get a clearer picture.
But we also have additional information which might be useful for understanding potential slowness - that is the <code class="language-plaintext highlighter-rouge">data locality level</code>,
data locality indicates data access latency.
Possible values will be:</p>

<ul>
  <li>PROCESS_LOCAL - data co-located with the code in the same JVM</li>
  <li>NODE_LOCAL - data located on the same node</li>
  <li>NO_PREF - data with no preference for locality</li>
  <li>RACK_LOCAL - data on the same rack but on a different server</li>
  <li>ANY - Data located on other racks</li>
</ul>

<p><a href="/assets/post-images/2022-11-15-spark-ui/data-per-task-per-executor.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/data-per-task-per-executor.JPG" alt="Metrics per task, per executor" /></a></p>

<h3 id="the-main-course---understanding-sql-tab">The main course - Understanding SQL tab</h3>
<p>In my opinion this is the most important part to understand since it gives us a clear picture of the application flow,
how much data was moved, how it was moved, which functionality was executed and how long any of them took.
Before examining the logical and physical plan of our application, make sure to toggle on the
<code class="language-plaintext highlighter-rouge">Show the Stage ID and Task ID that corresponds to the max metric</code> checkbox, this will help us to correlate parts of the plan
to bottlenecks we saw in the jobs and stages details.</p>

<h4 id="scanning-parquet">Scanning Parquet</h4>

<p><a href="/assets/post-images/2022-11-15-spark-ui/sql-1.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/sql-1.JPG" alt="Scan Parquet" /></a></p>

<p>First, we can see the <code class="language-plaintext highlighter-rouge">Scan parquet</code> stage which show information about our file structure in HDFS and how long it took the application to
obtain it (Listing leaf and files job).
We can see the number of partitions and files read, both stating 244 - which makes sense since our data was partitioned by country
with a single parquet file per partition:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/hue.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/hue.JPG" alt="Hue" /></a></p>

<p>We can see how long it took to complete the scan (1.2s, 5.6s) and number of output rows.
If we hover over this stage we can see additional info such as the directory, schema, and filters that were pushed:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/sql-1-1.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/sql-1-1.JPG" alt="Directory and schema" /></a></p>

<h4 id="wholestagecodegen---fuse-of-multiple-operators">WholeStageCodeGen - fuse of multiple operators</h4>
<p><a href="/assets/post-images/2022-11-15-spark-ui/sql-2.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/sql-2.JPG" alt="WholeStageCodeGen" /></a></p>

<p>WholeStageCodegen fuses multiple operators together into a single Java function that is aimed at improving execution performance.
It collapses a query into a single optimized function that eliminates virtual function calls and leverages CPU registers for intermediate data.
In our case, we can see that fuse of 3 different functions into a single WholeStageCodegen:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">ColumnarToRow</code> - which generates spark data frame from parquet</li>
  <li><code class="language-plaintext highlighter-rouge">Filter</code> - which was pushed on the <code class="language-plaintext highlighter-rouge">name</code> column by spark due to the join that will come next</li>
  <li><code class="language-plaintext highlighter-rouge">BroadcatHashJoin</code> - which joins <code class="language-plaintext highlighter-rouge">citizens</code> and <code class="language-plaintext highlighter-rouge">cities</code> data together (via broadcast due to the relatively small data size)</li>
</ol>

<p>In both <code class="language-plaintext highlighter-rouge">Filter</code> and <code class="language-plaintext highlighter-rouge">BroadcastHashJoin</code> we can conveniently see the number of output rows after a specific operator applied.</p>

<p>Next, we will see the <code class="language-plaintext highlighter-rouge">Project</code> operator, which simply represents what columns will be selected.
In our case, we will see the product of the join between the 2 dataframes:</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/product-of-join.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/product-of-join.JPG" alt="Project" /></a></p>

<h4 id="exchange">Exchange</h4>
<p>Next, we will see an <code class="language-plaintext highlighter-rouge">exchange</code> in the plan which was caused by our <code class="language-plaintext highlighter-rouge">repartition</code> command.
<a href="/assets/post-images/2022-11-15-spark-ui/exchange.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/exchange.JPG" alt="Exchange" /></a></p>

<p>If we hover over the box, we can see the partition method that was used (round-robin).
We can also see the total data size, number of partitions, records and various other metrics.</p>

<p>An important thing to notice here (and in the previous step of <code class="language-plaintext highlighter-rouge">BroadcastHashJoin</code>) is the number of output rows of the join.
If the number of the output rows is disproportional or doesn’t make sense from your understanding of the data, it might indicate that
there’s something wrong with your data, such as unexpected duplicates in the join column, or your assumptions about the data are incorrect.</p>

<h4 id="narrow-transformation">Narrow transformation</h4>
<p><a href="/assets/post-images/2022-11-15-spark-ui/next_year_and_write.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/next_year_and_write.JPG" alt="Exchange" /></a></p>

<p>Here you can see a <code class="language-plaintext highlighter-rouge">WholeStageCodegen</code> generated for our citizen multiplication command.</p>

<p>Please note that the duration we see in the WholeStageCodegen (1.3m, see image below) is combining the execution of the repartition command <em>and</em> the column value manipulation.
If we were to remove the repartition command, the column value manipulation would’ve moved to the top <code class="language-plaintext highlighter-rouge">WholeStageCodegen</code> function alongside the filter and the broadcast join.</p>

<p><a href="/assets/post-images/2022-11-15-spark-ui/narrow-duration.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/narrow-duration.JPG" alt="Exchange" /></a></p>

<h4 id="saving-the-output">Saving the output</h4>
<p><a href="/assets/post-images/2022-11-15-spark-ui/writing.JPG" target="_blank"><img src="/assets/post-images/2022-11-15-spark-ui/writing.JPG" alt="Exchange" /></a></p>

<p>In the end, we see <code class="language-plaintext highlighter-rouge">InsertIntoHadoopFsRelationCommand</code>, which representing the writing of the output back to parquet.</p>

<p>Among a lot of interesting metrics, there are few which are performance significant:</p>
<ol>
  <li>Number of output rows &amp; data size - this is kind of self-explanatory.</li>
  <li>Number of written files - this is important to notice, since constantly creating a lot of small files might impact the performance of whatever reads your data. A good file size will be 512MiB or 1GiB.</li>
</ol>


  </div><a class="u-url" href="/jacobshapira.github.io/jekyll/update/2022/11/14/performance-related-info-spark-ui.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/jacobshapira.github.io/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Jacob Shapira</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Jacob Shapira</li><li><a class="u-email" href="mailto:jacobamerz at gmail">jacobamerz at gmail</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>

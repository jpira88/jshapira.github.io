<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>LLM fine-tuning concepts and hyperparameters simplified | jshapira Software architecture, engineering and AI</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="LLM fine-tuning concepts and hyperparameters simplified" />
<meta name="author" content="j.shapira" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="/pending/2023-09-18-llm-fine-tuning-hypterparameters-simplified.html" />
<meta property="og:url" content="/pending/2023-09-18-llm-fine-tuning-hypterparameters-simplified.html" />
<meta property="og:site_name" content="jshapira Software architecture, engineering and AI" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-09-18T12:31:29+03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="LLM fine-tuning concepts and hyperparameters simplified" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@j.shapira" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"j.shapira"},"dateModified":"2023-09-18T12:31:29+03:00","datePublished":"2023-09-18T12:31:29+03:00","headline":"LLM fine-tuning concepts and hyperparameters simplified","mainEntityOfPage":{"@type":"WebPage","@id":"/pending/2023-09-18-llm-fine-tuning-hypterparameters-simplified.html"},"url":"/pending/2023-09-18-llm-fine-tuning-hypterparameters-simplified.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/styles.css">
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="jshapira | Software architecture, engineering and AI" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">jshapira | Software architecture, engineering and AI</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/pending/2023-09-18-llm-fine-tuning-hypterparameters-simplified.html">LLM fine-tuning concepts and hyperparameters simplified</a><a class="page-link" href="/tag/FE/">FE</a><a class="page-link" href="/tag/JS/TS/">JS/TS</a><a class="page-link" href="/tag/BE/">BE</a><a class="page-link" href="/tag/PHP/">PHP</a><a class="page-link" href="/tag/Performance/">Performance</a><a class="page-link" href="/tag/DataEngineering/">DataEngineering</a><a class="page-link" href="/tag/Java/">Java</a><a class="page-link" href="/tag/Python/">Python</a><a class="page-link" href="/tag/Architecture/">Architecture</a><a class="page-link" href="/tag/Devops/">Devops</a><a class="page-link" href="/tag/Cloud/">Cloud</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">LLM fine-tuning concepts and hyperparameters simplified</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-09-18T12:31:29+03:00" itemprop="datePublished">Sep 18, 2023
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">j.shapira</span></span></p>
  </header>

  
  
  
  

  <div class="post-content e-content" itemprop="articleBody">
    <p>At the time of writing this entry, the tech world is caught up in what I like to call a LLM-BOOM.<br />
Everyone’s implementing LLM based features, new models announced on an hourly basis and the LLM ecosystem, both open source and proprietary is exploding.<br />
One of the advantages of this high paced environment is that tools are getting better and easier to use.</p>

<p>In this entry I want to focus on fine tuning.<br />
If fine-tuning an LLM model was not a very straightforward task a year ago, now you can run step-by-step tools such as
<a href="https://github.com/Lightning-AI/lit-gpt" target="_blank">lit-gpt</a>.<br />
But in order to use this tool (or any other for that matter) effectively, you need to properly configure it, and in order to properly configure it,
you need to understand what each configuration means and have sufficient general knowledge about what you’re doing.</p>

<p>The purpose of this entry is to simplify explanation about concepts and hyperparameters related to fine tuning in terms anyone can understand.<br />
The explanations might be slightly over-simplified for experienced data scientist, but the target audience of this entry are professionals (such as software architects and engineers etc’) which
aren’t specialized in the field, but required to have some solid understanding for their daily work.</p>

<p>I will use lit-gpt as an example tool due to Lightning AI’s decision to choose simplicity over clean code, which makes the code easy to understand
and explain. But the concepts and parameters this entry will cover are generic to many fine tuning tools out there.</p>

<h3 id="data-sets-training-validation-and-test">Data sets: Training, Validation, and Test</h3>
<p>The first thing you’ll be required to do is to split your data set into training and test data set.
Many times you will also have a validation data set.
The optimal train/validation/test split depends on your data, but standard splits can be 80/10/10, 70/15/15 or 60/20/20.</p>

<p><em>Training data</em> is pretty self-explanatory, this is the data you’ll be using to train your model.</p>

<p><em>Validation data</em> used to automatically or manually optimize hyperparameters during training phase.
For example, every N intervals, we can validate the model performance with a loss function.
A loss function quantifies the difference between the predicated output as stated in the validation samples and the actual output produced by the model.
If we (manually or automatically) notice that the loss function stops decreasing (or stops increasing), we might want to adjust the hyperparameters.</p>

<p><em>Test data</em> is the final data on which we will test the model performance.</p>

<h3 id="tensors-and-pytorch-models">Tensors and PyTorch models</h3>
<p>As you’ll see once you run the preparation script, you’ll get <code class="language-plaintext highlighter-rouge">train.pt</code> and <code class="language-plaintext highlighter-rouge">test.pt</code> file in your <code class="language-plaintext highlighter-rouge">/data</code> folder.
<code class="language-plaintext highlighter-rouge">.pt</code> files are saved PyTorch models. PyTorch models can store various data, such as weights, tokens, simple python objects and tensors.
A <code class="language-plaintext highlighter-rouge">PyTorch Tensor</code> is basically a generic multidimensional array containing numerical values on which we can perform computational actions.</p>

<p>These <code class="language-plaintext highlighter-rouge">.pt</code> files are pytorch models representing</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">{</span>
<span class="s">'instruction'</span><span class="p">:</span> <span class="s">"My instruction..."</span><span class="p">,</span>
<span class="s">'input'</span><span class="p">:</span> <span class="s">'My input...'</span><span class="p">,</span>
<span class="s">'output'</span><span class="p">:</span> <span class="s">"My output..."</span><span class="p">,</span>
<span class="s">'input_ids'</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">19028</span><span class="p">,</span> <span class="mi">304</span><span class="p">,</span> <span class="mi">267</span><span class="p">,</span> <span class="mi">10522</span><span class="p">,</span> <span class="mi">325</span><span class="p">,</span> <span class="mi">11117</span><span class="p">,</span> <span class="mi">241</span><span class="p">,</span> <span class="mi">4957</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">14687</span><span class="p">,</span> <span class="p">,</span> <span class="p">...</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
<span class="s">'input_ids_no_response'</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">19028</span><span class="p">,</span> <span class="mi">304</span><span class="p">,</span> <span class="mi">267</span><span class="p">,</span> <span class="mi">10522</span><span class="p">,</span> <span class="mi">325</span><span class="p">,</span> <span class="mi">248</span><span class="p">,</span> <span class="mi">2726</span><span class="p">,</span> <span class="p">...</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
<span class="s">'labels'</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">19028</span><span class="p">,</span> <span class="mi">304</span><span class="p">,</span> <span class="mi">267</span><span class="p">,</span> <span class="mi">10522</span><span class="p">,</span> <span class="mi">325</span><span class="p">,</span> <span class="mi">11117</span><span class="p">,</span> <span class="mi">241</span><span class="p">,</span> <span class="mi">4957</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">14687</span><span class="p">,</span> <span class="p">,</span> <span class="p">...</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="p">}</span></code></pre></figure>

<p>These files contain pytorch objects with textual and tokenized representation of each instruction in your data set.
Later, these files will loaded and used to fine tune and test the model.</p>

<h3 id="evaluation-intervals-and-iterations">Evaluation intervals and iterations</h3>

  </div><a class="u-url" href="/pending/2023-09-18-llm-fine-tuning-hypterparameters-simplified.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">jshapira | Software architecture, engineering and AI</li><li><a class="u-email" href="https://twitter.com/shapira_j">https://twitter.com/shapira_j</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
